{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Found 1 collection(s):\n",
      "\n",
      "üìÅ Collection: markdown_documents\n",
      "üìù Sample documents:\n",
      "\n",
      "  1. üÜî ID: 20110118195823_001.md_chunk_20\n",
      "     üìé Metadata: {'source_file': '20110118195823_001.md'}\n",
      "     üìÑ Content: CALIFORNIA 94520 ###### LU TELEX: 67-5570 TELEPHONE: (510)798-2940 `TELEFAX: (909) 592-6920 OR (909) 592-3399` TELEFAX: (510) 798-2944 2, ALL DOCUMENTS SHALL BE SENT TO THE FOLLOWING: `-DOCUMENTS WITH QUOTATION` -TO PURCHASING AGENT `J. BIGONY - SAN DIHAS OFFICE` `-ORDER PROGRESS DOCUMENTS` -TO PURCHASING AGENT `J. BIGONY` `- SAN DIMAS` `OFFICE` -SUB-ORDERS -TO PURCHASING AGENT J. BIGONY [\"] SAN DIMAS OFFICE -ALL OTHER COMMUNICATION -TO PURCHASING AGENT J. BIGONY - SAN DIMAS OFFICE -DOCUMENTS FO...\n",
      "\n",
      "  2. üÜî ID: gei100752.md_chunk_38\n",
      "     üìé Metadata: {'source_file': 'gei100752.md'}\n",
      "     üìÑ Content: a navigation link labeled \"Reports,\" which suggests that clicking on this link will lead to a page displaying links to all configured reports. Here is the transcription of all visible text: ``` Historian Home Page - Windows Internet Explorer Click Reports. The page displays with links to all configured reports. ```The image appears to be a screenshot of a webpage with instructions on how to display reports using the Historian web browser. Here's the transcription of the text: --- **Click to disp...\n",
      "\n",
      "  3. üÜî ID: 20110118195823_001.md_chunk_23\n",
      "     üìé Metadata: {'source_file': '20110118195823_001.md'}\n",
      "     üìÑ Content: DETAILS OF COMPONENTS. 4.1NSPECTION DATES (STAGE AND FINAL INSPECTION) ARE TO BE ARRANGED WITH PURCHASER=S INSPECTION MANAGER. ``` SUB-ORDERS (SEE PAGE i, SECTION 4.3) ``` **VENDOR SHALL FORWARD TO PURCHASER UNPRICED COPIES OF ALL SUB-ORDERS SPECIFYING:** ###### 1.TNE PURCHASE ORDER NUMBER AND ITEM/TAG NUHBER OF WRICH MATERIAL HAS BEEN PURCHASED. 2.THE DATE OF DELIVERY. 3.PROOF THAT THE MATERIAL HAS BEEN ORDERED. ``` F. GENER]LL RE M3tRK S 1.ALL COPIES OF DOCUMENTS LARGER THAN \"A\" SIZE HAVE TO B...\n",
      "\n",
      "  4. üÜî ID: 20110118195823_001.md_chunk_21\n",
      "     üìé Metadata: {'source_file': '20110118195823_001.md'}\n",
      "     üìÑ Content: SUPPLY OF GO00S WILL BE CONSIDERED AS INCOHPLETE IF THE REQUIRED DOCUMENTS ARE NOT DELIVERED. 2.ALL CORRESPOHDENDE, DRAWINGS AND OTHER RELATED DOCUHENTS SHALL NAVE KTI‚ÄôS PURCHASE ORDER NUMBER AND ITEM/TAG NUMBER OF THE GOODS WHICH HAVE BEEN PURCHASED. 3.ALL DRAWINGS AND DOCUMENTS SHALL BE SUBMITTED UNDER A TRANSMITTAL LETTER. ! 4.ALL DOCUMENTS SHALL BE IN THE ENGLISH LANGUAGE. ###### 5.ALL UNITS AND DIMENSIONS SHALL BE IN ENGLISN/____UMITS. 6.REVIEW OF DRAWINGS AND OTHER DOCUMENTS BY KTt SHALL N...\n",
      "\n",
      "  5. üÜî ID: 20110907184700_001.md_chunk_11\n",
      "     üìé Metadata: {'source_file': '20110907184700_001.md'}\n",
      "     üìÑ Content: should indicate that on the bottom. Route from you to me and Ken Jackman, also copy Benny Owens. :jc ``` ----- ###### Memorandum ``` TO Geismar Managers Date August 13, 1980 ``` From `H. C. Stafford` Subject `Vacation` Copies WLSmith, FETranor, Reference ``` BCTruse, MJKaye I will be on vacation next week (Aug. 18-22). Please refer any items that need attention to W. L. Smith. H.C. ;afford :jc ``` ----- ##### Memorandum `To` `R. E. Harroun` Date `August ii, 1980` From `H.C. Stafford` Subject RES...\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Path to your ChromaDB directory\n",
    "CHROMA_DB_PATH = \"/shared_folders/team_1/austin/chroma_db\"\n",
    "\n",
    "# Connect to the ChromaDB client\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "\n",
    "# List and print all collection names\n",
    "collections = client.list_collections()\n",
    "print(f\"\\nüîç Found {len(collections)} collection(s):\\n\")\n",
    "\n",
    "for col_name in collections:\n",
    "    print(f\"üìÅ Collection: {col_name}\")\n",
    "\n",
    "    # Access the collection\n",
    "    collection = client.get_collection(name=col_name)\n",
    "\n",
    "    # Query the first few documents (n_results can be changed)\n",
    "    try:\n",
    "        results = collection.query(query_texts=[\"Show me documents\"], n_results=5)\n",
    "\n",
    "        docs = results.get(\"documents\", [[]])[0]\n",
    "        ids = results.get(\"ids\", [[]])[0]\n",
    "        metadatas = results.get(\"metadatas\", [[]])[0]\n",
    "\n",
    "        if docs:\n",
    "            print(\"üìù Sample documents:\\n\")\n",
    "            for idx, doc in enumerate(docs):\n",
    "                print(f\"  {idx + 1}. üÜî ID: {ids[idx]}\")\n",
    "                print(f\"     üìé Metadata: {metadatas[idx] if idx < len(metadatas) else 'None'}\")\n",
    "                print(f\"     üìÑ Content: {doc[:500]}{'...' if len(doc) > 500 else ''}\\n\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No documents found in this collection.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying collection '{col_name}': {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter a query (e.g. 'purchase orders', 'historian reports'): \")\n",
    "results = collection.query(query_texts=[query], n_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Entry 1\n",
      "{\n",
      "  \"id\": \"20111219094640186_0007.md_chunk_0\",\n",
      "  \"document\": \"The image appears to be an engineering drawing or blueprint, likely for a construction project. It includes various dimensions, details, and specifications related to the components of a structure. Here is a detailed description along with the transcribed text: ### Bill of Material | No. of Shipping Pieces | Piece Mark | Shape | Length | Remarks | WT. | |------------------------|------------|----------------------|---------|--------------|-----| | 1 | A36 | SCR Door Panels | 7 | | 0 | | | | L5 x\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"20111219094640186_0007.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.02710399590432644,\n",
      "    0.07315632700920105,\n",
      "    -0.023834863677620888,\n",
      "    -0.01162189431488514,\n",
      "    0.0229035671800375,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n",
      "üì¶ Entry 2\n",
      "{\n",
      "  \"id\": \"20110120151608_001_Material_Tests.md_chunk_0\",\n",
      "  \"document\": \"PAR.T HO: PRW#: A-~8009-1 Penetron~ Efg. ARO~OX Light Intons|ty g 15\\\" ....... N/A **Ponotront: 906/~03A** **Time 15** + 0! l~T ``` P/oa 98-009 WO: 90-009 P/~ AF~4OHIA O~JTL~ ``` 0~: A98009-1 **ACCePTaBLE** **6 CAU$ COU~UHABL~$** **~.0 HRS.** ``` OIST~I~UT%0~ ``` TSST/~ASTEES, lN~, REPAESE~/AtlVE ----- EXPQSURE DATA Energy Source T_ ,,. I,~,~. Size./V~Curies~FocalDlstance_~=~__MaterialThickness ,-/\\u2019-/._3 ,/~%.\\\" - Source-FiimArrangement ~S~\\u2019/.~. .~_. ~ o,~..-~. ~ ~, (r PenetrameterSize /.,,2/-~/.5. Film Sizes/Quantity 70 ,~, ~ x ~o \\\"\\\"/z./,_~. Type.~zSLL.~Z~_Singie I,...... ~ouble~Screens Front~ Ba~k ~.~-[/] # I_,f l // ###### Weld# Film ~ ~ ~ , l~l~t~lRl[~] Identification Number !~/~/~ Remarks ~\\\"-, ~7 ~ ~-2 / ##### zjz/ /-z. / / ###### .~. / / / .5 7-- f~ ~-.3 / / ``` ~-I / / z~& /-:z / <~\\u2019v~ ~ ,z-~ / #### / / ###### Time: /0. \\u2019~,~ ~TbM TO: ``` Inspector: \\\"~re~ .~ ~ . . / Asst: ~ ~\\u2019~ ~~. :~ / ----- **CLIENT:** **RE PORT NO:** 9\\\" ~ - RO. NO: q~oo~ **DATE:** -~\\\" /\\\" ~ 2\\u2019 JOB NO: **PAGE\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"20110120151608_001_Material_Tests.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.10044652223587036,\n",
      "    0.04919249191880226,\n",
      "    -0.06912560015916824,\n",
      "    0.046429600566625595,\n",
      "    0.003323699114844203,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n",
      "üì¶ Entry 3\n",
      "{\n",
      "  \"id\": \"20110120151608_001_Material_Tests.md_chunk_1\",\n",
      "  \"document\": \":~ / ----- **CLIENT:** **RE PORT NO:** 9\\\" ~ - RO. NO: q~oo~ **DATE:** -~\\\" /\\\" ~ 2\\u2019 JOB NO: **PAGE NO:** **~** **OF:** ###### Weld # Film Identification Number Remarks **I** **I** ----- ----- ## ! ``` DETAIL DETAIL VIEW ``` NOTE ``` (TYP} ``` DETAIL DETAIL LOCKWOOD GREENE / BASF ###### .i ``` SELECTIVE CATALYST REDUCTION ONIT A~ONIA INJECTION GRID ``` ----- ``` PLAN OF MANIFOLD AT HRSG LOCKWOOD GREENE / BASF SELECTIVE CATALYST REDUCT~ION UNIT ``` -----\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"20110120151608_001_Material_Tests.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.09583254903554916,\n",
      "    0.06860588490962982,\n",
      "    -0.03850240260362625,\n",
      "    -0.009128826670348644,\n",
      "    0.03491806238889694,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n",
      "üì¶ Entry 4\n",
      "{\n",
      "  \"id\": \"0001 Revision Record for Reference Manuals.md_chunk_0\",\n",
      "  \"document\": \"## COGENERATION UNIT II GEISMAR, LOUISIANA ### APPROPRIATION NO. M143 **PROJECT NO. 65-07~608** ### PLANT REFERENCE MANUAL INDEX Rev. 0 Rev I Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS ### BOOK 1 OF 20 Volume I Process Design 1.0 **0** ### Design Basis, Definition Package 2.0 **0** 1 ### Process Description 3.0 0 1 3 ### Process Control Narrative **4.0** 0 1 3 ## ~rocess Interlock List 0 ### Chemical Usage/Physical Properties !5.1 0 ## Amine/Phosphate 5.2 **0** ## Natural Gas 5.3 0 ### Waste Gas 6.0 ### Process Flow Diagrams 7.0 1 ### Safety Reviews 7.1 1 ## Step II Safety Review 7.2 1 ## Step III Safety Review Volume II - Design Basis/ ### Project Design Calcula- ## )ns/Technical Standards **1** ----- Rev. 0 Rev 1 Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS ## q 0 ### . ~pe Service Index 2.0 0 ### Pipeline List 3.0 0 ### Specialty Items BOOK 2 OF 20 4.0 0 1 **4** ### Mechanical and Process Calculations BOOK 3 OF 20 5.0 0 ## Piping Stress Calculations ### BOOK 4 OF 20. 5.0 **0** ## ping Stress\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"0001 Revision Record for Reference Manuals.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.052776992321014404,\n",
      "    0.0028351503424346447,\n",
      "    -0.03828005865216255,\n",
      "    -0.0846136286854744,\n",
      "    0.030934959650039673,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n",
      "üì¶ Entry 5\n",
      "{\n",
      "  \"id\": \"0001 Revision Record for Reference Manuals.md_chunk_1\",\n",
      "  \"document\": \"BOOK 3 OF 20 5.0 0 ## Piping Stress Calculations ### BOOK 4 OF 20. 5.0 **0** ## ping Stress Calculations ### tCont\\u2019d) BOOK 5A OF 20 6.0 0 ## Structural Calculations # 7.0 ### Electrical Calculations **8.0** 0 4 **5** ## Instrumentation Calculatlons 9.0 0 ## Technical Standards ### BOOK 5B OF 20 9.0 0 ## Technical Standards SM\\\\P:\\\\SPEC\\\\00285733\\\\Job book check lJst.doc ----- ### Rev. 0 Rev I Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS OOK 6 OF 20 ## Volume III Equipment Data **1.0** 0 ### Equipment List 2.0 ## Equipment Data **2.1** ### Z-1801 Heat Recovery Steam Generator **2.1.1** 0 ## Purchase Order including Change Orders 2.1.2 0 ## Purchase Requisition 2.1.3 0 2 ## HRSG Vendor Drawings **2.1.4** 0 ## Duct Burner Vendor Drawings ### OOK 7 OF 20 ## Volume III Equipment Data 2.1.5 0 ### Code Calculations 2.1.6 0 3 4 ### Performance Data BOOK 8 OF 20 ## Volume III Equipment Data 2.1.7 0 ## Operating, Maintenance and Installation Manuals ### BOOK 9 OF 20 ## Volume III Equipment Data **3** -----\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"0001 Revision Record for Reference Manuals.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.13918867707252502,\n",
      "    0.06879360228776932,\n",
      "    0.02496909722685814,\n",
      "    0.01958363503217697,\n",
      "    0.007624080404639244,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n",
      "üì¶ Entry 6\n",
      "{\n",
      "  \"id\": \"0001 Revision Record for Reference Manuals.md_chunk_2\",\n",
      "  \"document\": \"Maintenance and Installation Manuals ### BOOK 9 OF 20 ## Volume III Equipment Data **3** ----- ### Rev. 0 Rev I Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS ## 0 _ perating, Maintenance and Installation Manuals Cont. ### BOOK 10 OF 20 ## Volume III Equipment-Data **2.2** ### Z-1831 Gas Turbine Generator **2.2.1** 0 ## Purchase Order including # Change Orders : 2.2.2 0 ## Purchase Requisition **2.2.3** 0 ## Vendor Drawings ### BOOK 11 OF 20 ## Volume III Equipment Data 2.2.3 0 ## Vendor Drawings (Cont\\u2019d) ### BOOK 12 OF 20 ## Volume III Equipment Data **2.2.3** 0 2 ## Vendor Drawings (Cont\\u2019d) 2.2.4 0 2 ### Performance Data 2.2.5 0 ## Operating, Maintenance and ### Installation Manuals BOOK 13 A OF 20 ## Volume III Equipment Data ,,~ ### .-1941 HRSG Sample Panel **4** SM \\\\ P:\\\\SPEC\\\\ 00285733\\\\ job book check l~st.doc ----- ### Rev. 0 Rev I Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS 3.1 0 ## . urchase Order including Change Orders 2.3.2 0 ## Purchase Requisition ### 2.3.3 0 ## Vendor Drawings 2.3.4 0 ##\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"0001 Revision Record for Reference Manuals.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.08564288914203644,\n",
      "    0.04285353422164917,\n",
      "    -0.029831813648343086,\n",
      "    -0.028802871704101562,\n",
      "    -0.026997817680239677,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n",
      "üì¶ Entry 7\n",
      "{\n",
      "  \"id\": \"0001 Revision Record for Reference Manuals.md_chunk_3\",\n",
      "  \"document\": \"including Change Orders 2.3.2 0 ## Purchase Requisition ### 2.3.3 0 ## Vendor Drawings 2.3.4 0 ## Operating, Maintenance and ### Installation Manuals **2.4** ### Z-1961 Selective Catalytic Reduction (SCR) Unit **2.4.1** 0 3 4 ## Purchase Order including Change Orders 2.4.2 0 ## Purchase Requisition **2.4.3** 1 2 ## Vendor Drawings 2.4.4 1 ## perating, Maintenance and Installation Manuals **5** S M\\\\P:\\\\SPEC\\\\00285733\\\\ job book check lisLdoe ----- Rev. 0 Rev i Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS ### ,OOK 13 B OF 20 ## Volume III Equipment Data **2.4.4.** 3 ## Operating, Maintenance and Installation Manuals Cont.\\u2019d ### BOOK 13 C OF 20 ## Volume III Equipment Data **2.4.4** 3 ## Operating, Maintenance and Installation Manuals Con.t\\u2019d ### BOOK 13 D OF 20 ## Volume III Equipment Data # 3 ## Operating, Maintenance and Installation Manuals Con.t\\u2019d ### BOOK 14 OF 20 ## Volume III Equiplnent Data **2.5** ### C-1921A & C-1921B Nitrogen Compressors **2.5.1** 0 ## Purchase Order including Change\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"0001 Revision Record for Reference Manuals.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.06751102209091187,\n",
      "    0.004686539527028799,\n",
      "    0.027969280257821083,\n",
      "    -0.0421963632106781,\n",
      "    0.03463935852050781,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n",
      "üì¶ Entry 8\n",
      "{\n",
      "  \"id\": \"0001 Revision Record for Reference Manuals.md_chunk_4\",\n",
      "  \"document\": \"**2.5** ### C-1921A & C-1921B Nitrogen Compressors **2.5.1** 0 ## Purchase Order including Change Orders 2.5.2 **0** ## Purchase Requisition **2.5.3** 3 ## Vendor Drawings 2.5.4 0 ## Operating, Maintenance and ### T~stallation Manuals **6** ----- ### Rev. 0 Rev i Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS ## Jaop Fabricated Drums-and Tanks **2.6.1** 0 ## Purchase Order including Change Orders 2.6.2 0 ## Purchase Requisition 2.6.3 NA ## Operating, Maintenance and ### Installation Manuals **2.6.4** ### D-1921 Nitrogen Receiver **2.6.4.1** 0 ## Vendor Drawings **2.6.4.2** 0 ### Code Calculations ## 2.6.5 ### D-1931 Attemperator Water ## Storage Drum **2.6.5.1** 0 ### ,,ndor Drawings z.6.5.2 0 Code Calculations **2.6.6** ### TK-1841 Blowdown Tank **2.6.6.1** 0 ## Vendor Drawings 2.6.6.2 0 1 ### Code Calculations 2.6.7 0 ### TK-1911 & TK-1912 GT Area ## Sump Tanks 2.6.7.1 0 ## Vendor Drawings 2.6.7.2 0 ### Code Calculations **2.6.8** 0 ### TK-2981 HRSG LP Boiler ## Chemical Feed Day Tank\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"0001 Revision Record for Reference Manuals.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.09926725924015045,\n",
      "    0.039745621383190155,\n",
      "    -0.061094000935554504,\n",
      "    -0.07252166420221329,\n",
      "    -0.054954562336206436,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n",
      "üì¶ Entry 9\n",
      "{\n",
      "  \"id\": \"0001 Revision Record for Reference Manuals.md_chunk_5\",\n",
      "  \"document\": \"2.6.7.2 0 ### Code Calculations **2.6.8** 0 ### TK-2981 HRSG LP Boiler ## Chemical Feed Day Tank **2.6.8.1** 0 ## Purchase Requisition **7** SM \\\\ P:\\\\SPEC\\\\00285733\\\\job book check list.doc ----- ### Rev. 0 Rev I Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS ~.8.2 NA ### _,laerating, Maintenance and Installation Manuals **2.6.8.3** 0 3 ## Vendor Drawings **2.6.8.4** 0 ### Code Calculations 2.6.9 **0** ### TK-2982 HRSG HP Boiler ## Chemical Feed Day Tank 2.6.9.1 0 ## Vendor Drawings 2.7 ### Fuel Gas Heat Exchangers 2.7.1 0 ## Purchase Order including Change Orders 2.7.2 0 ## Purchase Requisition 2.7.3 NA ## Operating, Maintenance and stallation Manuals ### z.7.4 E-1811 NG Fuel Gas Heater 2.7.4.1 0 ## Vendor Drawings 2.7.4.2 0 ### Code Calculations 2.7.5 ### E-1821 WG Fuel Gas Heater 2.7.5.1 0 ## Vendor Drawings 2.7.5.2 0 ### Code Calculations BOOK 15 OF 20 ## Volume III Equipment Data **2,8** ### E-1931 Sweetwater Condenser **8** SM\\\\ P:\\\\S PEC\\\\ 00285733\\\\ job book check list.doe ----- ### Rev. 0 Rev I\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"0001 Revision Record for Reference Manuals.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.07591470330953598,\n",
      "    0.054158177226781845,\n",
      "    -0.009944191202521324,\n",
      "    0.002514188177883625,\n",
      "    -0.03255331516265869,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n",
      "üì¶ Entry 10\n",
      "{\n",
      "  \"id\": \"0001 Revision Record for Reference Manuals.md_chunk_6\",\n",
      "  \"document\": \"Sweetwater Condenser **8** SM\\\\ P:\\\\S PEC\\\\ 00285733\\\\ job book check list.doe ----- ### Rev. 0 Rev I Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS ~.1 0 ### o archase Order including ## Change Orders **2.8.2** 0 ### Purchase Requisition **2.8.3** 1 ## Vendor Drawings 2.8.4 2 ## Operating, Maintenance and ### Installation Manuals **2.8.5** 0 ### Code Calculations 2.9 ### Pumps **2.9.1** ### P-1911 GTG Area Sump Pump 2.9.1.1 0 ## Purchase Order including Change Orders 2.9.1.2 0 ## Purchase Requisition 9.1.3 1 ## Cendor Drawings ### 2.9.1.4 1 ## Operating, Maintenance and ### Installation Manuals ## 2.9.2 Feedwater Pumps 2.9.2.1 0 ## Purchase Order including Change Orders 2.9.2.2 0 ## Purchase Requisition **2.9.2.3** ### P-2533 HP Feedwater Pump No. 7 2.9.2.3.1 1 ## Vendor Drawings 2.9.2.3.2 0 ## Operating, Maintenance and ### Installation Manuals 2.9.2.4 ### -2571 Hot Condensate Pump ~No. 4 **9** ----- ### Rev. 0 Rev I Rev 2 Rev 3 Rev 4 Rev. 5 COMMENTS **\\\".2.4.1** 0 ## endor Drawings 2.9.2.4.2 0 ##\",\n",
      "  \"metadata\": {\n",
      "    \"source_file\": \"0001 Revision Record for Reference Manuals.md\"\n",
      "  },\n",
      "  \"embedding\": [\n",
      "    -0.07447202503681183,\n",
      "    -0.02131391316652298,\n",
      "    0.006606677547097206,\n",
      "    -0.05032891407608986,\n",
      "    -0.04243088513612747,\n",
      "    \"...\"\n",
      "  ]\n",
      "}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import json\n",
    "\n",
    "# Path to your ChromaDB\n",
    "CHROMA_DB_PATH = \"/shared_folders/team_1/austin/chroma_db\"\n",
    "\n",
    "# Connect to the collection\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = client.get_collection(\"markdown_documents\")\n",
    "\n",
    "# Get the first 10 entries with all fields\n",
    "results = collection.get(limit=10, include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
    "\n",
    "# Pretty-print each entry\n",
    "for i in range(len(results[\"ids\"])):\n",
    "    embedding = results[\"embeddings\"][i]\n",
    "    embedding_preview = embedding[:5].tolist() + [\"...\"] if embedding is not None else None\n",
    "\n",
    "    entry = {\n",
    "        \"id\": results[\"ids\"][i],\n",
    "        \"document\": results[\"documents\"][i],\n",
    "        \"metadata\": results[\"metadatas\"][i],\n",
    "        \"embedding\": embedding_preview\n",
    "    }\n",
    "\n",
    "    print(f\"üì¶ Entry {i + 1}\")\n",
    "    print(json.dumps(entry, indent=2))\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document data exploration...\n",
      "Connecting to ChromaDB...\n",
      "Retrieving sample of 200 document chunks...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids in get.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ragui_env/lib/python3.13/site-packages/chromadb/api/models/CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/ragui_env/lib/python3.13/site-packages/chromadb/api/models/CollectionCommon.py:242\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_get_request\u001b[0;34m(self, ids, where, where_document, include)\u001b[0m\n\u001b[1;32m    241\u001b[0m validate_filter_set(filter_set\u001b[38;5;241m=\u001b[39mfilters)\n\u001b[0;32m--> 242\u001b[0m validate_include(include\u001b[38;5;241m=\u001b[39minclude, dissalowed\u001b[38;5;241m=\u001b[39m[IncludeEnum\u001b[38;5;241m.\u001b[39mdistances])\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IncludeEnum\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ragui_env/lib/python3.13/site-packages/chromadb/api/types.py:729\u001b[0m, in \u001b[0;36mvalidate_include\u001b[0;34m(include, dissalowed)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(item \u001b[38;5;241m==\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m IncludeEnum):\n\u001b[0;32m--> 729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected include item to be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(IncludeEnum)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m     )\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dissalowed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(item \u001b[38;5;241m==\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m dissalowed):\n",
      "\u001b[0;31mValueError\u001b[0m: Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 269\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExploration complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 269\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[7], line 251\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting document data exploration...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Get document data\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m summary, results \u001b[38;5;241m=\u001b[39m explore_document_data(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Preview potential clusters\u001b[39;00m\n\u001b[1;32m    254\u001b[0m source_groups \u001b[38;5;241m=\u001b[39m cluster_preview(results)\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mexplore_document_data\u001b[0;34m(limit)\u001b[0m\n\u001b[1;32m     23\u001b[0m collection \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarkdown_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieving sample of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m document chunks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m results \u001b[38;5;241m=\u001b[39m collection\u001b[38;5;241m.\u001b[39mget(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m], limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Basic statistics\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Basic Statistics ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ragui_env/lib/python3.13/site-packages/chromadb/api/models/Collection.py:126\u001b[0m, in \u001b[0;36mCollection.get\u001b[0;34m(self, ids, where, limit, offset, where_document, include)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    104\u001b[0m     ids: Optional[OneOrMany[ID]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     include: Include \u001b[38;5;241m=\u001b[39m [IncludeEnum\u001b[38;5;241m.\u001b[39mmetadatas, IncludeEnum\u001b[38;5;241m.\u001b[39mdocuments],\n\u001b[1;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GetResult:\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get embeddings and their associate data from the data store. If no ids or where filter is provided returns\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    all embeddings up to limit starting at offset.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     get_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_get_request(\n\u001b[1;32m    127\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    128\u001b[0m         where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[1;32m    129\u001b[0m         where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    130\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[1;32m    131\u001b[0m     )\n\u001b[1;32m    133\u001b[0m     get_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_get(\n\u001b[1;32m    134\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    135\u001b[0m         ids\u001b[38;5;241m=\u001b[39mget_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_get_response(\n\u001b[1;32m    147\u001b[0m         response\u001b[38;5;241m=\u001b[39mget_results, include\u001b[38;5;241m=\u001b[39mget_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    148\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ragui_env/lib/python3.13/site-packages/chromadb/api/models/CollectionCommon.py:93\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     92\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/ragui_env/lib/python3.13/site-packages/chromadb/api/models/CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     92\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ragui_env/lib/python3.13/site-packages/chromadb/api/models/CollectionCommon.py:242\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_get_request\u001b[0;34m(self, ids, where, where_document, include)\u001b[0m\n\u001b[1;32m    239\u001b[0m     validate_ids(ids\u001b[38;5;241m=\u001b[39munpacked_ids)\n\u001b[1;32m    241\u001b[0m validate_filter_set(filter_set\u001b[38;5;241m=\u001b[39mfilters)\n\u001b[0;32m--> 242\u001b[0m validate_include(include\u001b[38;5;241m=\u001b[39minclude, dissalowed\u001b[38;5;241m=\u001b[39m[IncludeEnum\u001b[38;5;241m.\u001b[39mdistances])\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IncludeEnum\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set a data loader on the collection if loading from URIs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ragui_env/lib/python3.13/site-packages/chromadb/api/types.py:729\u001b[0m, in \u001b[0;36mvalidate_include\u001b[0;34m(include, dissalowed)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected include item to be a str, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(item \u001b[38;5;241m==\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m IncludeEnum):\n\u001b[0;32m--> 729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected include item to be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(IncludeEnum)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m     )\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dissalowed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(item \u001b[38;5;241m==\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m dissalowed):\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInclude item cannot be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(dissalowed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids in get."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data exploration script for BASF ChromaDB documents.\n",
    "This script analyzes the available document data to understand what fields and patterns exist.\n",
    "\"\"\"\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Path to your ChromaDB\n",
    "CHROMA_DB_PATH = \"/shared_folders/team_1/austin/chroma_db\"\n",
    "\n",
    "def explore_document_data(limit=100):\n",
    "    \"\"\"\n",
    "    Explore the document data in ChromaDB and return insights\n",
    "    about what fields and patterns exist.\n",
    "    \"\"\"\n",
    "    print(\"Connecting to ChromaDB...\")\n",
    "    client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "    collection = client.get_collection(\"markdown_documents\")\n",
    "    \n",
    "    print(f\"Retrieving sample of {limit} document chunks...\")\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\", \"embeddings\", \"ids\"], limit=limit)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\n=== Basic Statistics ===\")\n",
    "    print(f\"Total chunks retrieved: {len(results['ids'])}\")\n",
    "    \n",
    "    # Analyze document IDs\n",
    "    print(f\"\\n=== Document ID Patterns ===\")\n",
    "    id_patterns = []\n",
    "    for doc_id in results['ids']:\n",
    "        # Extract pattern by replacing digits with 'D' and letters with 'L'\n",
    "        pattern = ''.join('D' if c.isdigit() else 'L' if c.isalpha() else c for c in doc_id)\n",
    "        id_patterns.append(pattern)\n",
    "    \n",
    "    pattern_counts = Counter(id_patterns)\n",
    "    print(f\"Top ID patterns:\")\n",
    "    for pattern, count in pattern_counts.most_common(5):\n",
    "        print(f\"  {pattern}: {count} occurrences\")\n",
    "        print(f\"  Example: {next(id for id in results['ids'] if ''.join('D' if c.isdigit() else 'L' if c.isalpha() else c for c in id) == pattern)}\")\n",
    "    \n",
    "    # Analyze metadata fields\n",
    "    print(f\"\\n=== Metadata Analysis ===\")\n",
    "    metadata_fields = set()\n",
    "    for meta in results['metadatas']:\n",
    "        metadata_fields.update(meta.keys())\n",
    "    \n",
    "    print(f\"Available metadata fields: {', '.join(metadata_fields)}\")\n",
    "    \n",
    "    for field in metadata_fields:\n",
    "        values = [meta.get(field) for meta in results['metadatas'] if field in meta]\n",
    "        unique_values = set(values)\n",
    "        \n",
    "        print(f\"\\nField: {field}\")\n",
    "        print(f\"  Present in {len(values)}/{len(results['metadatas'])} metadata entries ({len(values)/len(results['metadatas'])*100:.1f}%)\")\n",
    "        print(f\"  Unique values: {len(unique_values)}\")\n",
    "        \n",
    "        if len(unique_values) <= 10:\n",
    "            # If there are only a few unique values, show them all\n",
    "            value_counts = Counter(values)\n",
    "            for val, count in value_counts.most_common():\n",
    "                print(f\"    {val}: {count} occurrences\")\n",
    "        else:\n",
    "            # Otherwise just show the most common ones\n",
    "            value_counts = Counter(values)\n",
    "            print(f\"  Most common values:\")\n",
    "            for val, count in value_counts.most_common(5):\n",
    "                print(f\"    {val}: {count} occurrences\")\n",
    "    \n",
    "    # Analyze source files\n",
    "    print(f\"\\n=== Source File Analysis ===\")\n",
    "    source_files = [meta.get('source_file', 'unknown') for meta in results['metadatas']]\n",
    "    unique_sources = set(source_files)\n",
    "    \n",
    "    print(f\"Total unique source files: {len(unique_sources)}\")\n",
    "    \n",
    "    # Find patterns in source file names\n",
    "    source_extensions = Counter([os.path.splitext(src)[1] for src in source_files if src != 'unknown'])\n",
    "    print(f\"Source file extensions:\")\n",
    "    for ext, count in source_extensions.most_common():\n",
    "        print(f\"  {ext if ext else '(no extension)'}: {count} occurrences\")\n",
    "    \n",
    "    # Look for date patterns in filenames\n",
    "    date_pattern = re.compile(r'(\\d{8})')  # Looking for YYYYMMDD pattern\n",
    "    files_with_dates = [f for f in source_files if date_pattern.search(f)]\n",
    "    print(f\"Files with date patterns: {len(files_with_dates)}/{len(source_files)} ({len(files_with_dates)/len(source_files)*100:.1f}%)\")\n",
    "    \n",
    "    if files_with_dates:\n",
    "        print(f\"Examples of files with dates:\")\n",
    "        for i, f in enumerate(files_with_dates[:5]):\n",
    "            print(f\"  {i+1}. {f}\")\n",
    "    \n",
    "    # Analyze document content\n",
    "    print(f\"\\n=== Document Content Analysis ===\")\n",
    "    doc_lengths = [len(doc) for doc in results['documents']]\n",
    "    \n",
    "    print(f\"Document length statistics:\")\n",
    "    print(f\"  Min: {min(doc_lengths)} characters\")\n",
    "    print(f\"  Max: {max(doc_lengths)} characters\")\n",
    "    print(f\"  Average: {sum(doc_lengths)/len(doc_lengths):.1f} characters\")\n",
    "    \n",
    "    # Check for common patterns in content\n",
    "    content_patterns = {\n",
    "        \"Contains table\": sum(1 for doc in results['documents'] if '|' in doc and '-|-' in doc),\n",
    "        \"Contains image reference\": sum(1 for doc in results['documents'] if 'image' in doc.lower() or 'figure' in doc.lower()),\n",
    "        \"Contains bullet points\": sum(1 for doc in results['documents'] if '* ' in doc or '- ' in doc),\n",
    "        \"Contains code blocks\": sum(1 for doc in results['documents'] if '```' in doc),\n",
    "        \"Contains headers\": sum(1 for doc in results['documents'] if re.search(r'#+\\s', doc))\n",
    "    }\n",
    "    \n",
    "    print(f\"Content patterns:\")\n",
    "    for pattern, count in content_patterns.items():\n",
    "        print(f\"  {pattern}: {count}/{len(results['documents'])} ({count/len(results['documents'])*100:.1f}%)\")\n",
    "    \n",
    "    # Analyze embeddings\n",
    "    print(f\"\\n=== Embedding Analysis ===\")\n",
    "    if results['embeddings'] and len(results['embeddings']) > 0:\n",
    "        embedding_dimensions = len(results['embeddings'][0])\n",
    "        print(f\"Embedding dimensions: {embedding_dimensions}\")\n",
    "        \n",
    "        # Calculate statistics on a sample embedding\n",
    "        sample_embedding = results['embeddings'][0]\n",
    "        print(f\"Sample embedding statistics:\")\n",
    "        print(f\"  Min value: {min(sample_embedding):.4f}\")\n",
    "        print(f\"  Max value: {max(sample_embedding):.4f}\")\n",
    "        print(f\"  Mean value: {sum(sample_embedding)/len(sample_embedding):.4f}\")\n",
    "    else:\n",
    "        print(\"No embeddings found in the retrieved data.\")\n",
    "    \n",
    "    # Return a structured summary\n",
    "    summary = {\n",
    "        \"total_chunks\": len(results['ids']),\n",
    "        \"metadata_fields\": list(metadata_fields),\n",
    "        \"unique_source_files\": len(unique_sources),\n",
    "        \"common_extensions\": dict(source_extensions.most_common(3)),\n",
    "        \"has_dates_in_filenames\": len(files_with_dates) > 0,\n",
    "        \"embedding_dimensions\": len(results['embeddings'][0]) if results['embeddings'] and len(results['embeddings']) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return summary, results\n",
    "\n",
    "def cluster_preview(results, n_chunks_per_source=3):\n",
    "    \"\"\"\n",
    "    Create a preview of how documents might cluster by source file\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Document Clustering Preview ===\")\n",
    "    \n",
    "    # Group by source file\n",
    "    source_groups = {}\n",
    "    for i, doc_id in enumerate(results['ids']):\n",
    "        source_file = results['metadatas'][i].get('source_file', 'unknown')\n",
    "        \n",
    "        if source_file not in source_groups:\n",
    "            source_groups[source_file] = []\n",
    "        \n",
    "        source_groups[source_file].append({\n",
    "            'id': doc_id,\n",
    "            'text_preview': results['documents'][i][:100] + '...' if len(results['documents'][i]) > 100 else results['documents'][i]\n",
    "        })\n",
    "    \n",
    "    # Print preview of a few source files and their chunks\n",
    "    print(f\"Preview of {min(5, len(source_groups))} source files and their chunks:\")\n",
    "    \n",
    "    for i, (source, chunks) in enumerate(list(source_groups.items())[:5]):\n",
    "        print(f\"\\nSource File {i+1}: {source}\")\n",
    "        print(f\"Total chunks: {len(chunks)}\")\n",
    "        \n",
    "        # Show a few chunks from this source\n",
    "        for j, chunk in enumerate(chunks[:n_chunks_per_source]):\n",
    "            print(f\"  Chunk {j+1} ({chunk['id']}):\")\n",
    "            print(f\"    {chunk['text_preview']}\")\n",
    "    \n",
    "    # Estimate how many unique documents we might have\n",
    "    print(f\"\\nEstimated document count: {len(source_groups)} unique source files\")\n",
    "    \n",
    "    return source_groups\n",
    "\n",
    "def analyze_source_filenames(source_files):\n",
    "    \"\"\"\n",
    "    Analyze patterns in source filenames to extract potential metadata\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Source Filename Pattern Analysis ===\")\n",
    "    \n",
    "    # Try to extract dates (format: YYYYMMDD)\n",
    "    date_pattern = re.compile(r'(\\d{8})')\n",
    "    dates = {}\n",
    "    \n",
    "    for source in source_files:\n",
    "        match = date_pattern.search(source)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            try:\n",
    "                year = int(date_str[:4])\n",
    "                month = int(date_str[4:6])\n",
    "                day = int(date_str[6:8])\n",
    "                \n",
    "                if 1990 <= year <= 2024 and 1 <= month <= 12 and 1 <= day <= 31:\n",
    "                    dates[source] = {\n",
    "                        'year': year,\n",
    "                        'month': month, \n",
    "                        'day': day,\n",
    "                        'full_date': f\"{year}-{month:02d}-{day:02d}\"\n",
    "                    }\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    print(f\"Found dates in {len(dates)}/{len(source_files)} filenames ({len(dates)/len(source_files)*100:.1f}%)\")\n",
    "    \n",
    "    if dates:\n",
    "        years = Counter([d['year'] for d in dates.values()])\n",
    "        print(f\"Years distribution:\")\n",
    "        for year, count in sorted(years.items()):\n",
    "            print(f\"  {year}: {count} files\")\n",
    "    \n",
    "    # Extract other potential metadata from filenames\n",
    "    words_in_filenames = []\n",
    "    for source in source_files:\n",
    "        # Remove date patterns and file extensions\n",
    "        clean_name = date_pattern.sub('', source)\n",
    "        clean_name = os.path.splitext(clean_name)[0]\n",
    "        \n",
    "        # Split by non-alphanumeric characters\n",
    "        words = re.findall(r'[a-zA-Z]{3,}', clean_name)\n",
    "        words_in_filenames.extend([w.lower() for w in words])\n",
    "    \n",
    "    common_words = Counter(words_in_filenames)\n",
    "    print(f\"\\nMost common words in filenames:\")\n",
    "    for word, count in common_words.most_common(15):\n",
    "        print(f\"  {word}: {count} occurrences\")\n",
    "    \n",
    "    return {\n",
    "        'dates': dates,\n",
    "        'common_words': dict(common_words.most_common(15))\n",
    "    }\n",
    "\n",
    "def save_results(summary, filename='document_data_summary.json'):\n",
    "    \"\"\"Save the summary results to a JSON file\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"\\nSummary saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the data exploration\"\"\"\n",
    "    print(\"Starting document data exploration...\")\n",
    "    \n",
    "    # Get document data\n",
    "    summary, results = explore_document_data(limit=200)\n",
    "    \n",
    "    # Preview potential clusters\n",
    "    source_groups = cluster_preview(results)\n",
    "    \n",
    "    # Analyze source filenames\n",
    "    source_files = list(source_groups.keys())\n",
    "    filename_analysis = analyze_source_filenames(source_files)\n",
    "    \n",
    "    # Add the filename analysis to the summary\n",
    "    summary['filename_analysis'] = filename_analysis\n",
    "    \n",
    "    # Save results\n",
    "    save_results(summary)\n",
    "    \n",
    "    print(\"\\nExploration complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragui_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

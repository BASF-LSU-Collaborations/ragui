{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ init_vector_db imported successfully.\n",
      "\n",
      "Connecting to database to inspect schema...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   Connected in 0.02s\n",
      "\n",
      "Inspecting tables in 'public' schema...\n",
      "   Found tables: spatial_ref_sys, vector_db, summary_vectors, mock_items_2, vector_db_2\n",
      "\n",
      "--- Schema for table: public.spatial_ref_sys ---\n",
      "   • srid                 INTEGER                   NOT NULL\n",
      "   • auth_name            VARCHAR(256)              NULLABLE\n",
      "   • auth_srid            INTEGER                   NULLABLE\n",
      "   • srtext               VARCHAR(2048)             NULLABLE\n",
      "   • proj4text            VARCHAR(2048)             NULLABLE\n",
      "\n",
      "--- Schema for table: public.vector_db ---\n",
      "   • id                   INTEGER                   NOT NULL\n",
      "   • chunkid              INTEGER                   NULLABLE\n",
      "   • description          TEXT                      NULLABLE\n",
      "   • category             TEXT                      NULLABLE\n",
      "   • md                   JSONB                     NULLABLE\n",
      "   • filepath             TEXT                      NULLABLE\n",
      "   • markdown             TEXT                      NULLABLE\n",
      "   • embedding            HALFVEC(768)              NULLABLE\n",
      "\n",
      "--- Schema for table: public.summary_vectors ---\n",
      "   • doc_id               INTEGER                   NOT NULL\n",
      "   • summary              TEXT                      NULLABLE\n",
      "   • file_path            TEXT                      NULLABLE\n",
      "   • embedding            VECTOR(384)               NULLABLE\n",
      "   • cluster_id           INTEGER                   NULLABLE\n",
      "   • cluster_name         TEXT                      NULLABLE\n",
      "\n",
      "--- Schema for table: public.mock_items_2 ---\n",
      "   • id                   INTEGER                   NOT NULL\n",
      "   • description          TEXT                      NULLABLE\n",
      "   • rating               INTEGER                   NULLABLE\n",
      "   • category             TEXT                      NULLABLE\n",
      "   • in_stock             BOOLEAN                   NULLABLE\n",
      "   • test_field           BOOLEAN                   NOT NULL\n",
      "   • md                   JSONB                     NULLABLE\n",
      "   • embedding            HALFVEC(1024)             NULLABLE\n",
      "\n",
      "--- Schema for table: public.vector_db_2 ---\n",
      "   • id                   INTEGER                   NOT NULL\n",
      "   • description          TEXT                      NULLABLE\n",
      "   • rating               INTEGER                   NULLABLE\n",
      "   • category             TEXT                      NULLABLE\n",
      "   • md                   JSONB                     NULLABLE\n",
      "   • file_path            TEXT                      NULLABLE\n",
      "   • markdown             TEXT                      NULLABLE\n",
      "   • embedding            HALFVEC(1024)             NULLABLE\n",
      "\n",
      "Database session closed.\n",
      "\n",
      "✅ Schema exploration finished.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Database Schema Exploration\n",
    "#\n",
    "# This notebook connects to the database and inspects the schemas of the tables\n",
    "# within the `public` schema.\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "from sqlalchemy import create_engine, text, inspect as sql_inspect\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path to point to your VDB pipeline location\n",
    "VDB_PIPELINE_PATH = \"/shared_folders/team_1/mark_vdb/vdb_pipeline\" #<-- ADJUST IF NEEDED\n",
    "\n",
    "# --- Add VDB pipeline to Python path ---\n",
    "if VDB_PIPELINE_PATH not in sys.path:\n",
    "    sys.path.append(VDB_PIPELINE_PATH)\n",
    "\n",
    "# --- Import database initializer ---\n",
    "try:\n",
    "    # Assuming init_vector_db returns session, engine\n",
    "    from init_vector_db import init_vector_db\n",
    "    INIT_DB_AVAILABLE = True\n",
    "    print(\"✅ init_vector_db imported successfully.\")\n",
    "except ImportError as e:\n",
    "    INIT_DB_AVAILABLE = False\n",
    "    warnings.warn(f\"⚠️ Failed to import init_vector_db. Check VDB_PIPELINE_PATH: {e}\")\n",
    "    # Define a placeholder if needed, or just rely on the check later\n",
    "    def init_vector_db(wipe_database=False): return None, None\n",
    "\n",
    "# --- Connect and Inspect Schema ---\n",
    "session = None\n",
    "engine = None\n",
    "\n",
    "print(\"\\nConnecting to database to inspect schema...\")\n",
    "connect_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if INIT_DB_AVAILABLE:\n",
    "        session, engine = init_vector_db(wipe_database=False)\n",
    "\n",
    "        if engine: # Check if connection was successful\n",
    "            print(f\"   Connected in {time.time() - connect_start_time:.2f}s\")\n",
    "            inspector = sql_inspect(engine)\n",
    "\n",
    "            # --- Get List of Tables ---\n",
    "            print(\"\\nInspecting tables in 'public' schema...\")\n",
    "            table_names = inspector.get_table_names(schema='public')\n",
    "\n",
    "            if not table_names:\n",
    "                print(\"   No tables found in the 'public' schema.\")\n",
    "            else:\n",
    "                print(f\"   Found tables: {', '.join(table_names)}\")\n",
    "\n",
    "                # --- Inspect Columns for Each Table ---\n",
    "                for table_name in table_names:\n",
    "                    print(f\"\\n--- Schema for table: public.{table_name} ---\")\n",
    "                    try:\n",
    "                        columns = inspector.get_columns(table_name, schema='public')\n",
    "                        if columns:\n",
    "                            for column in columns:\n",
    "                                col_name = column['name']\n",
    "                                col_type = str(column['type']) # Convert type object to string\n",
    "                                col_nullable = column['nullable']\n",
    "                                print(f\"   • {col_name:<20} {col_type:<25} {'NULLABLE' if col_nullable else 'NOT NULL'}\")\n",
    "                        else:\n",
    "                            print(\"      No columns found for this table.\")\n",
    "                    except Exception as col_error:\n",
    "                         print(f\"      ⚠️ Error inspecting columns for {table_name}: {col_error}\")\n",
    "\n",
    "        else:\n",
    "             print(\"   ❌ Failed to establish database connection via init_vector_db.\")\n",
    "\n",
    "    else:\n",
    "        print(\"   Skipping database connection as init_vector_db was not imported.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    # --- Close Connection ---\n",
    "    if session and session.is_active:\n",
    "        session.close()\n",
    "        print(\"\\nDatabase session closed.\")\n",
    "    elif engine: # If engine exists but session might not have been active\n",
    "         print(\"\\nDatabase connection closed (or session inactive).\")\n",
    "\n",
    "print(f\"\\n✅ Schema exploration finished.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Next Steps\n",
    "#\n",
    "# 1. Review the tables and their columns listed above.\n",
    "# 2. Identify the tables you want to explore further (e.g., `summary_vectors`).\n",
    "# 3. Create subsequent cells to query specific tables for row counts, sample data, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ init_vector_db imported successfully.\n",
      "\n",
      "Connecting to database to explore tables...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   Connected in 0.01s\n",
      "\n",
      "========== Exploring Table: public.vector_db ==========\n",
      "   ✅ Table 'public.vector_db' exists.\n",
      "   📊 Current row count: 47822\n",
      "\n",
      "   Fetching 3 sample rows...\n",
      "\n",
      "   📄 Sample data (3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>chunkid</th>\n",
       "      <th>category</th>\n",
       "      <th>md</th>\n",
       "      <th>filepath</th>\n",
       "      <th>embedding_preview</th>\n",
       "      <th>markdown_preview</th>\n",
       "      <th>description_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "      <td>/shared_folders/team_1/ben/cleaned_data/201110...</td>\n",
       "      <td>[-0.024612427,-0.05895996,-0.016738892,0.08184...</td>\n",
       "      <td>/shared_folders/team_1/document_batch/UTILS/Li...</td>\n",
       "      <td>the provided image appears to be a technical d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "      <td>/shared_folders/team_1/ben/cleaned_data/201110...</td>\n",
       "      <td>[0.01675415,-0.02835083,0.01361084,0.07159424,...</td>\n",
       "      <td>/shared_folders/team_1/document_batch/UTILS/Li...</td>\n",
       "      <td>the provided image appears to be a technical d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "      <td>/shared_folders/team_1/ben/cleaned_data/201109...</td>\n",
       "      <td>[0.028152466,-0.03466797,-0.04888916,0.0913085...</td>\n",
       "      <td>/shared_folders/team_1/document_batch/UTILS/Li...</td>\n",
       "      <td>the provided image appears to be a technical d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  chunkid category  md  \\\n",
       "0   1        0           {}   \n",
       "1   2        1           {}   \n",
       "2   3        2           {}   \n",
       "\n",
       "                                            filepath  \\\n",
       "0  /shared_folders/team_1/ben/cleaned_data/201110...   \n",
       "1  /shared_folders/team_1/ben/cleaned_data/201110...   \n",
       "2  /shared_folders/team_1/ben/cleaned_data/201109...   \n",
       "\n",
       "                                   embedding_preview  \\\n",
       "0  [-0.024612427,-0.05895996,-0.016738892,0.08184...   \n",
       "1  [0.01675415,-0.02835083,0.01361084,0.07159424,...   \n",
       "2  [0.028152466,-0.03466797,-0.04888916,0.0913085...   \n",
       "\n",
       "                                    markdown_preview  \\\n",
       "0  /shared_folders/team_1/document_batch/UTILS/Li...   \n",
       "1  /shared_folders/team_1/document_batch/UTILS/Li...   \n",
       "2  /shared_folders/team_1/document_batch/UTILS/Li...   \n",
       "\n",
       "                                 description_preview  \n",
       "0  the provided image appears to be a technical d...  \n",
       "1  the provided image appears to be a technical d...  \n",
       "2  the provided image appears to be a technical d...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Exploring Table: public.vector_db_2 ==========\n",
      "   ✅ Table 'public.vector_db_2' exists.\n",
      "   📊 Current row count: 0\n",
      "   ℹ️ Table is empty.\n",
      "\n",
      "Database session closed.\n",
      "\n",
      "✅ Table exploration finished.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Explore `vector_db` and `vector_db_2` Tables\n",
    "#\n",
    "# This notebook cell connects to the database and inspects the content\n",
    "# of the `public.vector_db` and `public.vector_db_2` tables by fetching\n",
    "# row counts and sample data.\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import text, inspect as sql_inspect\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path to point to your VDB pipeline location\n",
    "VDB_PIPELINE_PATH = \"/shared_folders/team_1/mark_vdb/vdb_pipeline\" #<-- ADJUST IF NEEDED\n",
    "SAMPLE_SIZE = 3 # How many sample rows to fetch from each table\n",
    "\n",
    "# --- Add VDB pipeline to Python path ---\n",
    "if VDB_PIPELINE_PATH not in sys.path:\n",
    "    sys.path.append(VDB_PIPELINE_PATH)\n",
    "\n",
    "# --- Import database initializer ---\n",
    "try:\n",
    "    from init_vector_db import init_vector_db\n",
    "    INIT_DB_AVAILABLE = True\n",
    "    print(\"✅ init_vector_db imported successfully.\")\n",
    "except ImportError as e:\n",
    "    INIT_DB_AVAILABLE = False\n",
    "    warnings.warn(f\"⚠️ Failed to import init_vector_db. Check VDB_PIPELINE_PATH: {e}\")\n",
    "    def init_vector_db(wipe_database=False): return None, None\n",
    "\n",
    "# --- Function to Explore a Table ---\n",
    "def explore_table(table_name, session, engine, inspector):\n",
    "    \"\"\"Fetches count and sample data for a given table.\"\"\"\n",
    "    print(f\"\\n{'='*10} Exploring Table: public.{table_name} {'='*10}\")\n",
    "    df_sample = pd.DataFrame()\n",
    "    row_count = 0\n",
    "\n",
    "    if not inspector.has_table(table_name, schema=\"public\"):\n",
    "        print(f\"   ❌ Table 'public.{table_name}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    print(f\"   ✅ Table 'public.{table_name}' exists.\")\n",
    "    try:\n",
    "        # Get row count\n",
    "        count_result = session.execute(\n",
    "             text(f\"SELECT COUNT(*) FROM public.{table_name};\")\n",
    "         ).scalar_one_or_none()\n",
    "        row_count = count_result if count_result is not None else 0\n",
    "        print(f\"   📊 Current row count: {row_count}\")\n",
    "\n",
    "        if row_count > 0:\n",
    "            # Get column names\n",
    "            columns_info = inspector.get_columns(table_name, schema=\"public\")\n",
    "            existing_columns = [col['name'] for col in columns_info] if columns_info else []\n",
    "\n",
    "            if existing_columns:\n",
    "                print(f\"\\n   Fetching {SAMPLE_SIZE} sample rows...\")\n",
    "                select_cols_str = \", \".join([f'\"{col}\"' for col in existing_columns]) # Quote names\n",
    "                sample_query = text(f\"\"\"\n",
    "                    SELECT {select_cols_str}\n",
    "                    FROM public.{table_name}\n",
    "                    ORDER BY id -- Assuming 'id' is the primary key or an indexed column\n",
    "                    LIMIT :limit;\n",
    "                \"\"\")\n",
    "                df_sample = pd.read_sql(sample_query, con=engine, params={'limit': SAMPLE_SIZE})\n",
    "\n",
    "                if not df_sample.empty:\n",
    "                    print(f\"\\n   📄 Sample data ({len(df_sample)} rows):\")\n",
    "                    # Display relevant columns, handle potentially large text/embedding\n",
    "                    cols_to_display = df_sample.columns.tolist()\n",
    "                    # Shorten embedding preview if it exists\n",
    "                    if 'embedding' in df_sample.columns:\n",
    "                        df_sample['embedding_preview'] = df_sample['embedding'].astype(str).str[:50] + '...'\n",
    "                        cols_to_display.remove('embedding')\n",
    "                        cols_to_display.append('embedding_preview')\n",
    "                    # Shorten markdown/description preview\n",
    "                    for col in ['markdown', 'description']:\n",
    "                         if col in df_sample.columns:\n",
    "                             df_sample[f'{col}_preview'] = df_sample[col].astype(str).str[:100] + '...'\n",
    "                             cols_to_display.remove(col)\n",
    "                             cols_to_display.append(f'{col}_preview')\n",
    "\n",
    "                    display(df_sample[[col for col in cols_to_display if col in df_sample.columns]]) # Show only existing cols\n",
    "                else:\n",
    "                    print(\"      Could not fetch sample rows.\")\n",
    "            else:\n",
    "                print(\"      Could not retrieve column names.\")\n",
    "        else:\n",
    "            print(\"   ℹ️ Table is empty.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Error exploring table {table_name}: {e}\")\n",
    "\n",
    "\n",
    "# --- Connect and Explore Tables ---\n",
    "session = None\n",
    "engine = None\n",
    "\n",
    "print(\"\\nConnecting to database to explore tables...\")\n",
    "connect_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if INIT_DB_AVAILABLE:\n",
    "        session, engine = init_vector_db(wipe_database=False)\n",
    "\n",
    "        if engine: # Check if connection was successful\n",
    "            print(f\"   Connected in {time.time() - connect_start_time:.2f}s\")\n",
    "            inspector = sql_inspect(engine)\n",
    "\n",
    "            # Explore the tables of interest\n",
    "            explore_table(\"vector_db\", session, engine, inspector)\n",
    "            explore_table(\"vector_db_2\", session, engine, inspector)\n",
    "\n",
    "        else:\n",
    "             print(\"   ❌ Failed to establish database connection via init_vector_db.\")\n",
    "\n",
    "    else:\n",
    "        print(\"   Skipping database connection as init_vector_db was not imported.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    # --- Close Connection ---\n",
    "    if session and session.is_active:\n",
    "        session.close()\n",
    "        print(\"\\nDatabase session closed.\")\n",
    "    elif engine:\n",
    "         print(\"\\nDatabase connection closed (or session inactive).\")\n",
    "\n",
    "print(f\"\\n✅ Table exploration finished.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Next Steps\n",
    "#\n",
    "# 1. Review the row counts and sample data for `vector_db` and `vector_db_2`.\n",
    "# 2. Compare their structure and content to `summary_vectors` and the output of `test.py`.\n",
    "# 3. This should clarify which table the `test.py` script is likely querying and what kind of data (e.g., document chunks vs. summaries) it contains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan_morse/miniconda3/envs/ragui_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VDB pipeline functions imported successfully.\n",
      "\n",
      "--- Testing VDB Pipeline Functions ---\n",
      "\n",
      "[1/4] Testing init_vector_db()...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   ✅ Connection successful in 0.01s\n",
      "\n",
      "[2/4] Testing search_vdb(query='I need help disassembling a dryer model 1600', num_results=3)...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "Time taken: 0.06195807456970215\n",
      "   ✅ search_vdb() completed in 0.26s\n",
      "   Returned 3 results.\n",
      "\n",
      "   --- Top Search Result Sample ---\n",
      "[\n",
      "    17786,\n",
      "    0.01639344262295082,\n",
      "    0.0,\n",
      "    0.01639344262295082,\n",
      "    \"/shared_folders/team_1/document_batch/UTILS/Library/AA Temp Backup Scans/Utilities Folders & Files Shelf 32/Air Dryer #3/20110627195344_001.PDF\",\n",
      "    \"/shared_folders/team_1/ben/cleaned_data/20110627195344_001.md\",\n",
      "    \"`` ` dryer specification & performance data sheet external heat reactivated dryer ppc model no . : t8100clb-s-4x-fii s.o . # : 1029694 cat . # : 1239433 standard operating conditions : fluid : air inlet flow rate : 6000 scfm ( referred to 70\\u00b0f and 14.7 psia ) inlet pressure ( prefilter ) : i00 psig inlet temperature : i00 of inlet moisture content : sat ( at specified inlet pressure ) outlet moisture content : -40 \\u00b0f dewpoint at line pressure detail specifications : operation : `` amloc `` moisture load sensing control drying time 4 hrs dryer cycle ( per nema std adi-1964 ) 8 hrs . regen . time 4 hrs adsorbent type activated alumina 4300 # /chamber heating 3 hrs purge flow 1709 scfm cooling 1 hrs purge source : atmospheric air ( by blower ) closed loop i00 psig dryer system design pressure 150 psig vessel design pressure 150 psig dryer system design temperature 400 \\u00b0f vessel design temperature 450\\u00b0f dryer pressure drop 3 psid ( max ) asme code stamped chamber relief valves dryer conn. size 8 `` flg \\u2019 d utilities : electrical nema type : 4x ( stainless stl ) heater steam usage 538 lb/hr input 460 .v , 6____q___0hz , 3 ph blower motor size 15 hp average control consumption 75 watts @ 120 .v , 6ohz , iph dryer control gas : 60 psig ( min ) dryer outlet 700\\u00b0f max . stm temp . cooling water flow 103 gpm cooling water temp ( inlet ) 95~f cooling water temp rise 15\\u00b0f instrumentation includes : purge flow indicator ch~4ber pressure gauges panel mtd pilot air heater outlet temp gauge purge pressure indicator gauges for each chamber temperature gauge inlet/outlet press gauge switching valve accessories model no . conn. size mounted prefilter pcci96008gi29 8 `` flg yes cartridge poci200su no drain valve pdv400 \\u00bd npt yes afterfilter pcci96008gi29 8 `` flg yes cartridge pcci200ht no `` ` -- -- - a technical specification and performance data sheet for a product , likely related to industrial equipment or machinery . here is a detailed analysis : # # # layout and design : - the document is structured into several sections , each with a title and detailed descriptions . - the text is organized in a tabular format with headers and rows of data . - there are no distinct colors or patterns ; the document is primarily black text on a white background . - the layout is clean and professional , typical of technical manuals or datasheets . # # # content analysis : 1 . * * title section * * : - the title at the top reads `` technical specification & performance data sheet . `` - below the title , there is a reference number `` t-1000-01 `` which suggests this is a specific version or revision of the document . 2 . * * product description * * : - the first section describes the product as a `` pump `` with various specifications such as flow rate , pressure , and temperature . - it includes details like `` net fluid flow , `` `` net fluid head , `` and `` net fluid power . `` 3 . * * performance characteristics * * : - this section lists performance characteristics such as `` net fluid flow , `` `` net fluid head , `` and `` net fluid power . `` - it also mentions `` net fluid power , `` `` net fluid efficiency , `` and `` net fluid volume . `` 4 . * * operational parameters * * : - this section includes operational parameters such as `` operating temp , `` `` operating pressure , `` and `` operating speed . `` - it also lists `` max operating temp , `` `` max operating pressure , `` and `` max operating speed . `` 5 . * * electrical specifications * * : - this section provides electrical specifications such as `` voltage , `` `` current , `` and `` power . `` - it includes details like `` input voltage , `` `` output voltage , `` and `` output current . `` 6 . * * additional information * * : - this section includes additional details such as `` maintenance , `` `` safety , `` and `` environmental . `` - it lists items like `` maintenance interval , `` `` safety features , `` and `` environmental requirements . `` 7 . * * accessories * * : - this section lists accessories that come with the product , such as `` filter , `` `` valve , `` and `` mounting . `` 8 . * * pneumatic products company * * : - `` ` date : june 3,1993 to : list from/location : j. w. grass subject : air dryer vendor drawings transmittal copies : list : r. p. gaudin r. h. hindmarch c. e. myers . attached for your use and reference are the latest vendor drawings for the air dryer that is currently on order with pneumatic products . `` ` -- -- - the provided image appears to be a scanned document , likely a letter or memo , rather than a technical drawing . here is a detailed description based on the visual elements : # # # layout and colors : - the document is primarily white with black text . - there is no significant use of color other than the black ink used for the text and signature . - the layout is simple and structured , resembling a standard business letter format . # # # text analysis : - the document contains a header that reads `` basf , `` indicating the company associated with the document . - the date at the top left corner is `` june 3 , 1992 . `` - the recipient is listed as `` 1000 . `` - the sender is identified as `` r . brown . `` - the subject line states : `` all dyne insertion ( dynergy transmissions ) . `` - the body of the letter includes a list of items or tasks , which appear to be related to the subject of the letter . these items are numbered and seem to be instructions or points of discussion . - at the bottom right , there is a signature , presumably of the sender , r. brown . # # # possible ocr context : the ocr text provided does not seem to match the content of the image . it appears to be random characters or noise , possibly due to the quality of the scan or an error in the ocr process . therefore , we will not include it in our detailed description . # # # step-by-step explanation : 1 . * * header * * : the document starts with the company name `` basf `` at the top right corner . 2 . * * date * * : the date `` june 3 , 1992 ``\"\n",
      "]\n",
      "\n",
      "[3/4] Testing get_embeddings()...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   ✅ get_embeddings() completed in 1.64s\n",
      "   Returned 47822 embeddings.\n",
      "   Type of first embedding: <class 'sqlalchemy.engine.row.Row'>\n",
      "\n",
      "[4/4] Regarding add_entries()...\n",
      "   This function is used to add documents and their embeddings to the database.\n",
      "   Based on the documentation, it requires a path to a JSON file.\n",
      "   Each entry in the JSON should be a dictionary containing:\n",
      "     - 'real_path': Path to the original source file.\n",
      "     - 'file_path': Path to the corresponding markdown file (used for embedding).\n",
      "   Example usage (NOT RUN):\n",
      "   # from add_documents_vdb import add_entries\n",
      "   # try:\n",
      "   #     session, _ = init_vector_db()\n",
      "   #     add_entries('/path/to/your/data.json') # Requires session handling within add_entries\n",
      "   #     print('Data added successfully.')\n",
      "   # except Exception as e:\n",
      "   #     print(f'Error adding entries: {e}')\n",
      "   # finally:\n",
      "   #     if session: session.close()\n",
      "\n",
      "Database session closed.\n",
      "\n",
      "✅ VDB Pipeline function tests finished.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Test VDB Pipeline Functions\n",
    "#\n",
    "# This cell tests the core functions (`init_vector_db`, `search_vdb`, `get_embeddings`)\n",
    "# described in the pipeline documentation, primarily interacting with the `vector_db` table\n",
    "# based on the structure suggested by the `test.py` script output.\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from sqlalchemy import text, inspect as sql_inspect\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path to point to your VDB pipeline location\n",
    "VDB_PIPELINE_PATH = \"/shared_folders/team_1/mark_vdb/vdb_pipeline\" #<-- ADJUST IF NEEDED\n",
    "# Test query from test.py (or use another relevant query)\n",
    "TEST_SEARCH_QUERY = \"I need help disassembling a dryer model 1600\"\n",
    "# TEST_SEARCH_QUERY = \"chemical reactions in catalytic processes\"\n",
    "NUM_SEARCH_RESULTS = 3 # Number of results to fetch in the search test\n",
    "\n",
    "# --- Add VDB pipeline to Python path ---\n",
    "if VDB_PIPELINE_PATH not in sys.path:\n",
    "    sys.path.append(VDB_PIPELINE_PATH)\n",
    "\n",
    "# --- Import VDB pipeline functions ---\n",
    "try:\n",
    "    from init_vector_db import init_vector_db\n",
    "    from search_vdb import search_vdb, get_embeddings\n",
    "    # Assuming 'vector' model is not needed directly for these tests\n",
    "    # from vector import vector\n",
    "    # from variables import MODEL, NUM_OF_SEARCH_RESULTS # Import if needed by functions\n",
    "\n",
    "    INIT_DB_AVAILABLE = True\n",
    "    SEARCH_AVAILABLE = True\n",
    "    GET_EMB_AVAILABLE = True\n",
    "    print(\"✅ VDB pipeline functions imported successfully.\")\n",
    "\n",
    "except ImportError as e:\n",
    "    warnings.warn(f\"⚠️ Failed to import one or more VDB functions: {e}\")\n",
    "    # Set flags to false if imports fail\n",
    "    if 'init_vector_db' not in locals(): INIT_DB_AVAILABLE = False\n",
    "    if 'search_vdb' not in locals(): SEARCH_AVAILABLE = False\n",
    "    if 'get_embeddings' not in locals(): GET_EMB_AVAILABLE = False\n",
    "    # Define placeholders to avoid crashing later if imports failed\n",
    "    def init_vector_db(wipe_database=False): return None, None\n",
    "    def search_vdb(query, num_results=3): return []\n",
    "    def get_embeddings(): return []\n",
    "\n",
    "\n",
    "# --- Test Functions ---\n",
    "session = None\n",
    "engine = None\n",
    "\n",
    "print(\"\\n--- Testing VDB Pipeline Functions ---\")\n",
    "\n",
    "# --- 1. Test init_vector_db() ---\n",
    "print(\"\\n[1/4] Testing init_vector_db()...\")\n",
    "connect_start_time = time.time()\n",
    "try:\n",
    "    if INIT_DB_AVAILABLE:\n",
    "        session, engine = init_vector_db(wipe_database=False)\n",
    "        if engine:\n",
    "            print(f\"   ✅ Connection successful in {time.time() - connect_start_time:.2f}s\")\n",
    "            # Optional: Check connection status further if needed\n",
    "            # print(f\"   Engine dialect: {engine.dialect.name}\")\n",
    "        else:\n",
    "            print(\"   ❌ init_vector_db() returned None for engine. Connection failed.\")\n",
    "    else:\n",
    "        print(\"   Skipping test: init_vector_db not imported.\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Error during init_vector_db(): {e}\")\n",
    "\n",
    "# --- 2. Test search_vdb() ---\n",
    "print(f\"\\n[2/4] Testing search_vdb(query='{TEST_SEARCH_QUERY}', num_results={NUM_SEARCH_RESULTS})...\")\n",
    "search_results = []\n",
    "if engine and SEARCH_AVAILABLE: # Proceed only if connected and function imported\n",
    "    search_start_time = time.time()\n",
    "    try:\n",
    "        # Assuming search_vdb uses the session implicitly or connects itself\n",
    "        # If it requires the session, pass it: search_vdb(session, TEST_SEARCH_QUERY, num_results=NUM_SEARCH_RESULTS)\n",
    "        search_results = search_vdb(TEST_SEARCH_QUERY, num_results=NUM_SEARCH_RESULTS)\n",
    "\n",
    "        print(f\"   ✅ search_vdb() completed in {time.time() - search_start_time:.2f}s\")\n",
    "        print(f\"   Returned {len(search_results)} results.\")\n",
    "        print(\"\\n   --- Top Search Result Sample ---\")\n",
    "        if search_results:\n",
    "            # Print the first result in a readable format\n",
    "            first_hit = search_results[0]\n",
    "            print(json.dumps(first_hit, indent=4))\n",
    "            # You can add more detailed printing like in test.py if needed\n",
    "        else:\n",
    "            print(\"   search_vdb() returned no results for this query.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error during search_vdb(): {e}\")\n",
    "elif not engine:\n",
    "     print(\"   Skipping test: Database connection failed.\")\n",
    "else: # Not SEARCH_AVAILABLE\n",
    "     print(\"   Skipping test: search_vdb not imported.\")\n",
    "\n",
    "\n",
    "# --- 3. Test get_embeddings() ---\n",
    "print(\"\\n[3/4] Testing get_embeddings()...\")\n",
    "all_embeddings = []\n",
    "if engine and GET_EMB_AVAILABLE: # Proceed only if connected and function imported\n",
    "    get_emb_start_time = time.time()\n",
    "    try:\n",
    "        # Assuming get_embeddings uses the session implicitly or connects itself\n",
    "        # If it requires the session, pass it: get_embeddings(session)\n",
    "        all_embeddings = get_embeddings()\n",
    "        print(f\"   ✅ get_embeddings() completed in {time.time() - get_emb_start_time:.2f}s\")\n",
    "        print(f\"   Returned {len(all_embeddings)} embeddings.\")\n",
    "        if all_embeddings:\n",
    "            # Check the type and shape/length of the first embedding\n",
    "            first_emb = all_embeddings[0]\n",
    "            print(f\"   Type of first embedding: {type(first_emb)}\")\n",
    "            try:\n",
    "                # Check length if it's list-like, or shape if numpy array\n",
    "                if isinstance(first_emb, (list, tuple)):\n",
    "                     print(f\"   Length of first embedding: {len(first_emb)}\")\n",
    "                elif hasattr(first_emb, 'shape'): # Check for numpy array shape\n",
    "                     print(f\"   Shape of first embedding: {first_emb.shape}\")\n",
    "                # Add more type checks if needed\n",
    "            except TypeError:\n",
    "                 print(\"   Could not determine length/shape of the first embedding.\")\n",
    "        else:\n",
    "            print(\"   get_embeddings() returned no embeddings (table might be empty or function targets wrong table).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error during get_embeddings(): {e}\")\n",
    "elif not engine:\n",
    "     print(\"   Skipping test: Database connection failed.\")\n",
    "else: # Not GET_EMB_AVAILABLE\n",
    "     print(\"   Skipping test: get_embeddings not imported.\")\n",
    "\n",
    "\n",
    "# --- Close Session ---\n",
    "if session and session.is_active:\n",
    "    session.close()\n",
    "    print(\"\\nDatabase session closed.\")\n",
    "elif engine:\n",
    "    print(\"\\nDatabase connection closed (or session inactive).\")\n",
    "\n",
    "print(f\"\\n✅ VDB Pipeline function tests finished.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Next Steps\n",
    "#\n",
    "# 1. Review the output of the tests above.\n",
    "# 2. Did `search_vdb` return relevant results for the query? Does the structure match the `test.py` output?\n",
    "# 3. Did `get_embeddings` return the expected number of embeddings? Does the dimension match the `vector_db` schema (`HALFVEC(768)`)?\n",
    "# 4. This helps confirm if the pipeline functions are interacting with the intended table (`vector_db`) correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VDB pipeline functions imported successfully.\n",
      "\n",
      "--- Testing search_vdb() ---\n",
      "Connecting to database...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   Connected in 0.01s\n",
      "\n",
      "Performing search for: 'I need help disassembling a dryer model 1600' (Top 5 results)\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "Time taken: 0.06718182563781738\n",
      "   Search completed in 0.09s\n",
      "   Found 5 results.\n",
      "\n",
      "--- Search Results (Formatted JSON) ---\n",
      "[\n",
      "    [\n",
      "        17786,\n",
      "        0.01639344262295082,\n",
      "        0.0,\n",
      "        0.01639344262295082,\n",
      "        \"/shared_folders/team_1/document_batch/UTILS/Library/AA Temp Backup Scans/Utilities Folders & Files Shelf 32/Air Dryer #3/20110627195344_001.PDF\",\n",
      "        \"/shared_folders/team_1/ben/cleaned_data/20110627195344_001.md\",\n",
      "        \"`` ` dryer specification & performance data sheet external heat reactivated dryer ppc model no . : t8100clb-s-4x-fii s.o . # : 1029694 cat . # : 1239433 standard operating conditions : fluid : air inlet flow rate : 6000 scfm ( referred to 70\\u00b0f and 14.7 psia ) inlet pressure ( prefilter ) : i00 psig inlet temperature : i00 of inlet moisture content : sat ( at specified inlet pressure ) outlet moisture content : -40 \\u00b0f dewpoint at line pressure detail specifications : operation : `` amloc `` moisture load sensing control drying time 4 hrs dryer cycle ( per nema std adi-1964 ) 8 hrs . regen . time 4 hrs adsorbent type activated alumina 4300 # /chamber heating 3 hrs purge flow 1709 scfm cooling 1 hrs purge source : atmospheric air ( by blower ) closed loop i00 psig dryer system design pressure 150 psig vessel design pressure 150 psig dryer system design temperature 400 \\u00b0f vessel design temperature 450\\u00b0f dryer pressure drop 3 psid ( max ) asme code stamped chamber relief valves dryer conn. size 8 `` flg \\u2019 d utilities : electrical nema type : 4x ( stainless stl ) heater steam usage 538 lb/hr input 460 .v , 6____q___0hz , 3 ph blower motor size 15 hp average control consumption 75 watts @ 120 .v , 6ohz , iph dryer control gas : 60 psig ( min ) dryer outlet 700\\u00b0f max . stm temp . cooling water flow 103 gpm cooling water temp ( inlet ) 95~f cooling water temp rise 15\\u00b0f instrumentation includes : purge flow indicator ch~4ber pressure gauges panel mtd pilot air heater outlet temp gauge purge pressure indicator gauges for each chamber temperature gauge inlet/outlet press gauge switching valve accessories model no . conn. size mounted prefilter pcci96008gi29 8 `` flg yes cartridge poci200su no drain valve pdv400 \\u00bd npt yes afterfilter pcci96008gi29 8 `` flg yes cartridge pcci200ht no `` ` -- -- - a technical specification and performance data sheet for a product , likely related to industrial equipment or machinery . here is a detailed analysis : # # # layout and design : - the document is structured into several sections , each with a title and detailed descriptions . - the text is organized in a tabular format with headers and rows of data . - there are no distinct colors or patterns ; the document is primarily black text on a white background . - the layout is clean and professional , typical of technical manuals or datasheets . # # # content analysis : 1 . * * title section * * : - the title at the top reads `` technical specification & performance data sheet . `` - below the title , there is a reference number `` t-1000-01 `` which suggests this is a specific version or revision of the document . 2 . * * product description * * : - the first section describes the product as a `` pump `` with various specifications such as flow rate , pressure , and temperature . - it includes details like `` net fluid flow , `` `` net fluid head , `` and `` net fluid power . `` 3 . * * performance characteristics * * : - this section lists performance characteristics such as `` net fluid flow , `` `` net fluid head , `` and `` net fluid power . `` - it also mentions `` net fluid power , `` `` net fluid efficiency , `` and `` net fluid volume . `` 4 . * * operational parameters * * : - this section includes operational parameters such as `` operating temp , `` `` operating pressure , `` and `` operating speed . `` - it also lists `` max operating temp , `` `` max operating pressure , `` and `` max operating speed . `` 5 . * * electrical specifications * * : - this section provides electrical specifications such as `` voltage , `` `` current , `` and `` power . `` - it includes details like `` input voltage , `` `` output voltage , `` and `` output current . `` 6 . * * additional information * * : - this section includes additional details such as `` maintenance , `` `` safety , `` and `` environmental . `` - it lists items like `` maintenance interval , `` `` safety features , `` and `` environmental requirements . `` 7 . * * accessories * * : - this section lists accessories that come with the product , such as `` filter , `` `` valve , `` and `` mounting . `` 8 . * * pneumatic products company * * : - `` ` date : june 3,1993 to : list from/location : j. w. grass subject : air dryer vendor drawings transmittal copies : list : r. p. gaudin r. h. hindmarch c. e. myers . attached for your use and reference are the latest vendor drawings for the air dryer that is currently on order with pneumatic products . `` ` -- -- - the provided image appears to be a scanned document , likely a letter or memo , rather than a technical drawing . here is a detailed description based on the visual elements : # # # layout and colors : - the document is primarily white with black text . - there is no significant use of color other than the black ink used for the text and signature . - the layout is simple and structured , resembling a standard business letter format . # # # text analysis : - the document contains a header that reads `` basf , `` indicating the company associated with the document . - the date at the top left corner is `` june 3 , 1992 . `` - the recipient is listed as `` 1000 . `` - the sender is identified as `` r . brown . `` - the subject line states : `` all dyne insertion ( dynergy transmissions ) . `` - the body of the letter includes a list of items or tasks , which appear to be related to the subject of the letter . these items are numbered and seem to be instructions or points of discussion . - at the bottom right , there is a signature , presumably of the sender , r. brown . # # # possible ocr context : the ocr text provided does not seem to match the content of the image . it appears to be random characters or noise , possibly due to the quality of the scan or an error in the ocr process . therefore , we will not include it in our detailed description . # # # step-by-step explanation : 1 . * * header * * : the document starts with the company name `` basf `` at the top right corner . 2 . * * date * * : the date `` june 3 , 1992 ``\"\n",
      "    ],\n",
      "    [\n",
      "        1857,\n",
      "        0.0,\n",
      "        0.01639344262295082,\n",
      "        0.01639344262295082,\n",
      "        \"/shared_folders/team_1/document_batch/UTILS/Library/Equipment and Systems/4000 Air/Air Dryers/No 2 Plant Air Dryer/20101118214306_001.PDF\",\n",
      "        \"/shared_folders/team_1/ben/cleaned_data/20101118214306_001.md\",\n",
      "        \"item description # for dryer model 1600 qty . qty . valve assembly unless absolutely necessary . ( see notes ) 1 valve body 1 2 valve flange ( see notes ) 2 2 3 actuator subassembly 1 * * _warning : ensure that the dryer is de-_ * * 4 valve seat ( see notes ) 2 2 _energized , valve-isolated and fully_ 5 connector 2 2 _depressurized before attempting to_ 6 ring gasket ( seat ) 2 ( see notes ) _remove or disassemble any dryer-_ 7 ring gasket ( body ) 2 2 _related component or subassembly._ 8 o-ring ( connector ) 2 2 _failure to do so may result in_ 9 threaded stud 4 4 _serious personal injury and / or_ 10 hex nut 8 8 _equipment damage._ 11 spacer 4 4 _actuator subassembly , see figure 3-4b_ 12 position indicator 1 1 12a roll pin 1 1 12b pivot pin 1 i # # ! . disconnect pilot air tubing and position indicator cables . 12c o-ring ( pivot pin ) 1 1 remove valve from the pipe manifolds . 12d retaining plate 1 1 12e mounting adapter 1 1 2. clean and inspect valve seats and poppets for damage 12f magnet 1 1 and excessive wear . use mirror . do not disassemble 12g set screw 1 1 valve at this time . manually apply pressure to the pop- 12h sensor housing 1 1 12j machine screw 2 2 pet and push it back and forth several times . if a # # # tendency to bind or erratic operation is noted , disassemble and repair the valve . refer to figures chart a 3-4a and 3-4b for instructions for disassembly and valve valvelocknut connector stud size seat reassembly . ~ 30-35 ff-lbs 40-45 ft-lbs . 180-200 ft4bs . 80-95 flqbs * * disassembly / assembly instructions * * do not disassemble flanges and valve seats notes unless absolutely necessary . 1. disassemble / assemble the flanges and the valve seats . disassembled to service valve proper . a ) clean and degrease the threads on the flanges and valve seats . # # # b ) insert gasket in flange . deta/l d c ) apply loctite [ \\u00ae ] rc-620 to valve seats . d ) tighten valve seats to torque specified in chart a . 2. disassemble / build the valve position indicator in the order shown in the valve assembly diagram . note : when installed properly , the north pole of the magnet ( marked by red paint ) will point towards the hall effect switches . # # # 3. disassemble / build the valve assembly in the order _detail c_ * * _warning : do not apply pilot pres-_ * * shown in the valve assembly diagram . _note_ _sure to the valve without valve_ a ) invert the valve body as shown in detail b . * * notes : in early design valves the seat * * _flanges and seats in place . failure_ b ) carefully insert the roll pin into the slot of the ( item 4 ) was threaded into the _to have the valve flanges and_ dome cover . valve flange ( item 2 ) with a ring _sea ts in place when pilot pressure_ # # # c ) support the actuator subassembly , install the con- gasket ( item 6 ) . is applied may result in serious nectors . _personal injury and / or equipment_ note : o-rings should be lightly lubricated to prevent later designs have the seat _damage._ shearing . ( item 4 ) welded to the valve d ) tighten connectors to torque specified in chart a. flange ( item 2 ) . e ) lockwire connectors as shown detail c. f ) hand tighten studs in sequence shown in detail d. g ) tighten studs in sequence shown to torque specified in chart a . * * switching valve * * * * for dryer model 1600 * * h ) test valve . # # # figure 3-4a -- -- - the provided image appears to be a technical drawing , likely a schematic or assembly guide for a mechanical component , possibly a valve or a similar device . here is a detailed breakdown : # # # layout and design : - * * page layout * * : the page is divided into two main sections . on the left side , there are several smaller diagrams and text blocks , while the right side features a larger , more detailed diagram of the assembled part . - * * color scheme * * : the background is white , which helps the black lines and text stand out clearly . there are no other colors present except for the black lines and text . - * * text * * : there is text present throughout the image , providing instructions , labels , and descriptions . some of the text is legible through optical character recognition ( ocr ) . # # # detailed analysis : # # # # left side : - * * diagram 1 * * : this section shows a close-up view of a valve , labeled as `` 1/2 `` switching valve assembly . `` - * * diagram 2 * * : another close-up view , possibly showing different parts or angles of the same valve . - * * diagram 3 * * : a cross-sectional view of the valve , highlighting internal components . - * * text blocks * * : these blocks contain detailed instructions and labels for each part of the valve . for example , `` 1/2 `` switching valve assembly `` and `` 1/2 `` switching valve assembly `` appear multiple times , indicating repetition or emphasis . # # # # right side : - * * main diagram * * : this is a detailed exploded view of the valve assembly . it shows all the individual parts that make up the valve , numbered sequentially . each part is labeled with a number corresponding to the text blocks on the left side . - * * instructions * * : the text on the right side provides step-by-step instructions on how to assemble the valve . it includes references to specific parts and their placement within the assembly . # # # ocr text : some of the text is recognizable through ocr : - `` 1/2 `` switching valve assembly `` - `` 1/2 `` switching valve assembly `` - `` 1/2 `` switching valve assembly `` - `` 1/2 `` switching valve assembly `` - `` 1/2 `` switching valve assembly `` # # # possible interpretation : the image is a comprehensive guide for assembling a 1/2 `` switching valve . it includes detailed diagrams and text instructions that help the user understand the structure and assembly process of the valve . the repeated text on the left side emphasizes certain parts or steps , ensuring clarity and ease of use . # # # conclusion : the\"\n",
      "    ],\n",
      "    [\n",
      "        10129,\n",
      "        0.0,\n",
      "        0.016129032258064516,\n",
      "        0.016129032258064516,\n",
      "        \"/shared_folders/team_1/document_batch/UTILS/Library/Job Books-Projects-Studies/1998 Wastewater Plant Expansion/Vol II Book 4/Book 4 Tab 3.0/20110304164435_001.PDF\",\n",
      "        \"/shared_folders/team_1/ben/cleaned_data/20110304164435_001.md\",\n",
      "        \"h~vc t , h~gs d~=plc~ `` a rcco , , m , , m6ndat~on that t , ~e operator o~h~vk [ w ~== open di|~lf need be we can add a flow transmitter on the vent line going to tk-401 to alert the # # # # ~t6~ 6f ~ high f16~ ~it~ti~6 : ~ ~h~ij~n~ ~~6~ { 6 ~i~a ~ ~ ~p from pulling from air through dryer that is not operating . i don \\u2019 t think this is a show stopper , just another problem to be overcome . changing our basic philosophy of venting into tk-401 will be a very unpopular decision with safety ( ih ) end the operators . from : fergusson , caryle ( lge-at ) to : duvvuri , murthy s. cc : biddix , kristi ; mcdavid , olaf ; smith , kent ( lge-at ) ; morton , steve ( lge-at ) subject ; re : deodorizer date : wednesday , june 04 , 1997 5:33pm i talked to doug to clarify the information below . we discussed the design basis for the deodorizer as follows : they have done some `` deodorizer `` designs recently . typically , they use a venturi to remove solids , then a condensing tower to remove steam , then an `` odor control device `` like a caustic scrubber or thermal oxidizer , etc . the unit combination used depends on the components in the particular stream dried . the actual flow of gas+vapor from a dryer depends a lot upon the specific hookup and operation of the unit . for units of size similar to the basf installation , their `` field experience `` says 500 acfm is required per dryer , but this is a `` conservative , rough estimate `` . for a well operated , buttoned up unit , 300 acfm is closer to reality ; with a hatch open , 750 acfm is closer . they usually oversize their fans to be sure they `` don \\u2019 t balk `` when a hatch is open , resulting in a face full of steam for an operator . for our design , they would use about 2100 ib air + 1800 ib water vapor ( approximately ) per hour . the design basis for the deodorizer can be increased accordingly , but by routing this flow to tk-401 , there is an upper limit to the total flow through the unit ... , it can not exceed the capacity of bl-225 . - > > > > please call to discuss this , right now , the flow rate for the deodorizers seems to be too small , as currently designed , from : smith , kent ( lge-at ) to : biddix , kristi ; duvvuri , murthy s. ; fergusson , caryle ( lge-at ) ; mcdavid , olaf ; morton , steve ( lge-at ) subject : deodorizer date ; wednesday , june 04 , 1997 10:49am i confirmed the expected vapor flow rate and composition from the sludge dryers to the deodorizer with doug grunwald at komline-sanderson . mr. grunwald \\u2019 s number is 908-234-1000. the expected flow rate is as follows from each dryer : 2000 tbs/hr of water vapor 500 acfm of air page 2 -- -- - the provided image appears to be a scanned document or a technical drawing , but due to the low resolution and lack of clarity , it is challenging to discern specific details . however , based on the ocr text provided , we can infer that the document contains technical specifications or instructions related to a product or process . # # # detailed analysis : 1 . * * text content * * : - the ocr text suggests that the document includes detailed technical specifications or instructions for a product or process . it mentions terms like `` m4 , `` `` m5 , `` `` m6 , `` which could refer to specific types of screws or bolts commonly used in mechanical assemblies . - there are references to `` m4-20 , `` `` m5-20 , `` `` m6-20 , `` which likely denote the size and length of screws or bolts . - the text also includes measurements such as `` 100 mm , `` `` 150 mm , `` which could indicate dimensions relevant to the product or process being described . - there are mentions of `` screw , `` `` nut , `` `` washer , `` which are components typically found in mechanical assemblies . - the text seems to provide instructions or specifications for assembling or disassembling a device , possibly involving screws , nuts , washers , and other mechanical parts . 2 . * * layout and colors * * : - the document appears to be a single-page scan with a white background . - there are no distinct colors or patterns visible in the image . - the text is black , which is typical for printed documents . 3 . * * step-by-step explanation * * : - the document likely provides a series of steps or instructions for assembling or disassembling a mechanical device . - it may include a list of required components ( e.g . , screws , nuts , washers ) and their sizes . - the text might specify the order in which these components should be assembled or disassembled . - there could be diagrams or illustrations accompanying the text , although they are not clearly visible in the provided image . 4 . * * possible text recognition * * : - the ocr text is partially legible , but due to the low resolution , some words are difficult to decipher accurately . - the text includes technical terms and measurements , suggesting that it is a technical document . # # # context inclusion : the ocr text provides valuable information about the content of the document . it indicates that the document is likely a technical specification or instruction manual for a mechanical assembly process . the presence of terms like `` m4 , `` `` m5 , `` `` m6 # # # # fergusson , caryle ( lge-at ) * * from : * * smith , kent ( lge-at ) to : biddix , kristi ; duvvuri , murthy s. ; fergusson , caryle ( lge-at ) ; mcdavid , olaf ; morton , steve ( lge-at ) * * subject : * * deodorizer design basis per the conversation between ray perner , suri , and myself this morning , we will design each deodorizer , t-560 & t-550 , for 1050 acfm each ( 300 acfm for a buttoned-up dryer and 750 acfm for a dryer with the inspection door open ) . therefore , the worst case situation will occur when three dryers are operating at full capacity and one of the three has an inspection door open . dryer 1 300 acfm @ 212\\u00b0f dryer 2 300 acfm @ 212\\u00b0f dryer 3 750 acfm @ 212\\u00b0f ( inspection door open ) 1,350 acfm @\"\n",
      "    ],\n",
      "    [\n",
      "        19985,\n",
      "        0.016129032258064516,\n",
      "        0.0,\n",
      "        0.016129032258064516,\n",
      "        \"/shared_folders/team_1/document_batch/UTILS/TechTeam/4000 Air and Nitrogen/Projects/USE-206829 Geismar Plant Air Surge System/Compressor/Bids/Blackmer Bid/Blackmer HDL602C IOM.pdf\",\n",
      "        \"/shared_folders/team_1/ben/cleaned_data/Blackmer HDL602C IOM.md\",\n",
      "        \"washer and shims . keep the shims & piston together . e. repeat these steps for the other piston . 6. remove the cylinder capscrews . 7. the cylinder and cylinder o-rings can then be lifted from the crosshead guide ( or distance piece ) . 8. removal of the packing boxes is dependent on the model number of the compressor . for disassembly of the packing boxes , refer to `` seal ( packing ) replacement `` . double-seal models a. using an adjustable spanner wrench , remove the packing box hold-down rings . ( replace the nylon locking inserts in the hold-down rings during reassembly . ) b. the spacer rings , upper packing box o-rings , packing boxes and lower packing box o-rings can then be removed from the piston rods . triple-seal models a. lift the upper packing box assemblies and orings off the piston rods . b. remove the upper distance piece capscrews and lift the upper distance piece and o-rings off the crosshead guide . c. using an adjustable spanner wrench , remove the packing box hold-down rings . ( replace the nylon locking inserts in the hold-down rings during reassembly . ) d. the spacer rings , upper packing box o-rings , packing boxes and lower packing box o-rings can then be removed from the piston rods . 9. remove the crosshead guide capscrews , then lift the crosshead guide and gasket off . 10. to remove the connecting rod assemblies ( with the crossheads attached ) it may be necessary to drain the oil from the crankcase . note : the piston rod is permanently attached to the crosshead to form a single assembly . do not attempt disassembly . a. remove the crankcase inspection plate . b. remove the locknuts from the connecting rod bolts . this will release the connecting rod cap ( the lower half of the connecting rod ) and the two halves of the bearing insert . * * note : the connecting rod and the connecting * * rod cap are marked with a dot on one side so that they can be matched properly when reassembling . c. lift the crosshead assembly and connecting rod off the crankcase . * * note : the connecting rod parts are not * * * * interchangeable and must be reassembled * * * * with the same upper and lower halves . to * * * * avoid confusion , work on one connecting rod * * * * at a time , or mark the individual halves with * * * * corresponding numbers . * * 11. remove the opposite connecting rod and crosshead assembly in the same manner . 12. rest the crosshead assembly on a bench . carefully drive the wrist pin and wrist pin plugs out of the crosshead and connecting rod using a suitable pin driver or an arbor press . removal of the pin releases the crosshead assembly from the connecting rod . -- -- - a page from a manual or guide , likely related to maintenance or operation instructions for a piece of equipment or machinery . the text is structured into two main columns , each containing several numbered steps or instructions . # # # detailed description : 1 . * * layout and structure * * : - the page is divided into two main columns , each containing multiple steps or instructions . - each step is numbered sequentially , suggesting a step-by-step process . - the text is formatted in a clear , readable font , making it easy to follow the instructions . 2 . * * content * * : - the left column contains instructions that seem to be related to the operation or maintenance of an engine or similar mechanical component . - the right column provides additional details or explanations for the steps listed in the left column . - both columns use bullet points to list specific actions or considerations . 3 . * * visual elements * * : - there are small icons or symbols at the top of the left column , possibly indicating different types of instructions or warnings . - the page has a blue header at the top , which might contain the title or section heading of the document . 4 . * * language and format * * : - the language used is formal and technical , typical of manuals or instructional guides . - the format is consistent throughout the page , maintaining readability and ease of following the instructions . 5 . * * purpose * * : - based on the content and structure , this page is likely part of a larger manual designed to guide users through the proper operation or maintenance of a specific piece of equipment . # # # context ( ignored ) : - the context provided seems unrelated to the image and does not contribute any additional information about its content or purpose.13 . if necessary , the wrist pin needle bearings can be replaced after the crossheads are removed . the small end of the connecting rod is fitted with two roller bearings separated by a spacer . when properly installed , the roller bearings should protrude 0.075 `` ( 1.9 mm ) on each side of the conrod . 14. to replace the crankshaft bearings , the crankcase must be disassembled , and the crankshaft removed . refer to `` bearing replacement `` for disassembly instructions . # # # # # compressor assembly compressor assembly is generally the opposite of compressor disassembly . before reassembling , clean each part thoroughly . check all machined surfaces for burrs or roughness , and file lightly if necessary . * * replace * * * * any o-rings or gaskets that are removed or disturbed * * * * during service . * * # # # # # # 1. crankcase assembly after replacing the crankshaft , bearing carrier , and bearing cover plate ( see `` bearing replacement `` ) , the connecting rod and crosshead can be installed . a. to attach the connecting rod to the crosshead assembly , first coat the wrist pin , the wrist pin bore in the crosshead assembly , and the wrist pin bushing in the connecting rod with grease . b. start the wrist pin in the bore of the crosshead assembly and tap lightly until the pin begins to project through to the inside of the crosshead assembly . c. slide the connecting rod up inside of the crosshead assembly and align the bushing with the wrist pin . d. lightly tap the wrist pin through the connecting rod until it is centered in the crosshead assembly . note : the wrist pin should be snug in the crosshead assembly . the connecting rod should rotate freely on the wrist pin , but should not be loose . e. dip the wrist pin plugs in grease and press them in place . f. place the bearing halves into each half of the connecting rod , aligning the\"\n",
      "    ],\n",
      "    [\n",
      "        32279,\n",
      "        0.0,\n",
      "        0.015873015873015872,\n",
      "        0.015873015873015872,\n",
      "        \"/shared_folders/team_1/document_batch/UTILS/UtInfo/022 PSM-RMP and PRS/PSR - 20M-40M-50M/40M/Process Description/Reference Documents/Process overviews/UTIL_BH_SU_026_R0_START_AIR_DRYER_(Z-4411)_TRAINING_INFO.docx\",\n",
      "        \"/shared_folders/team_1/ben/cleaned_data/UTIL_BH_SU_026_R0_START_AIR_DRYER_(Z-4411)_TRAINING_INFO.md\",\n",
      "        \"|col1|col2| | -- -| -- -| ||| -- -- - a document titled `` start-up `` with a reference number `` 1.2-4013 `` . related to basf , as indicated by the logo and name at the top left corner . the content of the document includes sections such as `` purpose , `` `` safety , `` `` precautions , `` `` equipment , `` `` procedures , `` and `` reason for damage . `` each section has a corresponding number , suggesting that this might be a checklist or a form for documenting steps during a startup process . the document is structured with clear headings and numbered entries , indicating a systematic approach to the procedures described.|col1|col2| | -- -| -- -| ||| -- -- - a document titled `` start job 0999999-1 l2-451 ( l ) `` . related to basf corporation and its personal protective equipment ( ppe ) . it includes references to various documents such as the ppe chart , basf corporation electronic msds , and other relevant documents . the purpose of the document is described as detailing how to use type 1 , 2 ( 451 ( l ) ) after donning the equipment to ensure the quality of air being breathed by the wearer . the document is structured with sections labeled `` purpose `` and `` references `` . the references list several documents including the ppe chart , basf corporation electronic msds , and others . the document also mentions standards such as `` n95 `` , `` personal protective equipment `` , and `` basf corporation site operating procedures `` . the ocr text suggests that the document might be part of a larger set of instructions or guidelines for using specific types of personal protective equipment in an industrial setting . the presence of references to msds ( material safety data sheets ) indicates that safety information is crucial for the users of this equipment.|col1|col2| | -- -| -- -| ||| -- -- - a document titled `` basf `` with a subtitle `` start-up 04997 `` and a reference number `` 1.2-4033 `` . related to procedures for operators , specifically for `` procedures number 1775 - startup operations . `` the content includes sections such as `` precautions , `` `` special equipment , `` and `` prerequisites . `` under `` precautions , `` it states that if an operator encounters operating issues while performing the specified procedure , they should notify the supervisor to review and modify the procedure . under `` special equipment , `` it lists items such as personal protective equipment ( ppe ) requirements list ( ppe requirements list ) , ppe grid , and other equipment used in the procedure . the `` prerequisites `` section includes items like `` received attachment a - air purge of 4033 ( 2 ) information and attachments b ( 1 ) control devices included in this procedure . `` the document is formatted with clear headings and bullet points , suggesting it is a structured guide for operators to follow during startup operations.|col1|col2| | -- -| -- -| ||| -- -- - a document titled `` basf `` with a subtitle `` start air dryer ( p-12-a0453 ) `` . a procedural guide for operating an air dryer system . it includes a section labeled `` procedure `` and another labeled `` caution `` . the procedure outlines steps for starting and operating the air dryer , including : 1. do not allow air being drawn into the system or vent to blow directly on your skin or onto your face . 2. high pressure air could contain debris which could cause injury . 3. close all drain valves on the system . 4. when the air dryer is pressurized , open the bypass valve on air dryer . 5. close the bypass valve on air dryer . 6. open the bypass valve on p-12-a0453 main blower . 7. set the bypass valve on p-12-a0453 to 100 % to ready the system . the caution section advises against allowing air to be drawn into the system or vent to blow directly on the skin or face due to potential high pressure air containing debris that could cause injury . the document is structured to ensure safety and proper operation of the air dryer system.|col1|col2| | -- -| -- -| ||| -- -- - a document titled `` start air dryer no . 4 ( 443-2 ) l-24-4033 . `` it seems to be an instruction sheet for operating an air dryer , specifically model number 443-2. the document includes several numbered steps that outline the process of starting and verifying the operation of the air dryer . each step contains specific instructions such as : 1. verify that all trips are in the regulation air dryer ( a-443-2 ) . 2. verify that the air dryer bypass is operational . 3. verify that the air dryer bypass is operational . 4. verify that the air dryer bypass is operational . 5. turn on the main power and local control to the air dryer operating panel . there is also a cautionary note at the bottom of the document , which states : * * caution : * * failure to make proper adjustments may cause process units of an air dryer to proceed without problems . the document is structured in a clear , step-by-step format , designed to guide the operator through the startup procedure of the air dryer.|col1|col2| | -- -| -- -| ||| -- -- - a document titled `` start air dryer no . 1 `` from basf . the document includes a series of instructions and procedures for operating an air dryer , specifically air dryer no . 1. the text is structured into several sections , each detailing specific steps or checks that need to be performed . key elements of the document : - * * title * * : start air dryer no . 1 - * * company * * : basf - * * procedure number * * : 1.2-4053 - * * date * * : 11/06 - * * page * * : page 2 of 2 the document outlines the following steps : 1 . * * initial preparation * * : - ensure the dryer is at room temperature . - check the filter and replace if necessary . - ensure the dryer is clean . 2 . * * starting the dryer * * : - turn on the dryer . - allow the dryer to warm up by running through one regeneration cycle . 3 . * * maintenance checks * * : - perform daily checks as detailed in the `` daily inspection `` section . 4 . * * daily inspection * * : - check the pressure gauge . - check the filter . - check the oil level . - check the oil quality . the document is formatted in a clear , step-by-step manner , typical of operational manuals used in industrial settings . it emphasizes the importance\"\n",
      "    ]\n",
      "]\n",
      "\n",
      "Database session closed.\n",
      "\n",
      "✅ Search test finished.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Using the `search_vdb()` Function\n",
    "#\n",
    "# This cell demonstrates how to use the `search_vdb()` function from the pipeline\n",
    "# to perform a hybrid search on the `vector_db` table and view the results.\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path to point to your VDB pipeline location\n",
    "VDB_PIPELINE_PATH = \"/shared_folders/team_1/mark_vdb/vdb_pipeline\" #<-- ADJUST IF NEEDED\n",
    "# Define the search query you want to test\n",
    "SEARCH_QUERY = \"I need help disassembling a dryer model 1600\"\n",
    "# Define how many results you want\n",
    "NUM_RESULTS = 5\n",
    "\n",
    "# --- Add VDB pipeline to Python path ---\n",
    "if VDB_PIPELINE_PATH not in sys.path:\n",
    "    sys.path.append(VDB_PIPELINE_PATH)\n",
    "\n",
    "# --- Import VDB pipeline functions ---\n",
    "try:\n",
    "    from init_vector_db import init_vector_db\n",
    "    from search_vdb import search_vdb\n",
    "    INIT_DB_AVAILABLE = True\n",
    "    SEARCH_AVAILABLE = True\n",
    "    print(\"✅ VDB pipeline functions imported successfully.\")\n",
    "except ImportError as e:\n",
    "    warnings.warn(f\"⚠️ Failed to import one or more VDB functions: {e}\")\n",
    "    if 'init_vector_db' not in locals(): INIT_DB_AVAILABLE = False\n",
    "    if 'search_vdb' not in locals(): SEARCH_AVAILABLE = False\n",
    "    # Define placeholders\n",
    "    def init_vector_db(wipe_database=False): return None, None\n",
    "    def search_vdb(query, num_results=3): return []\n",
    "\n",
    "# --- Connect, Search, and Display Results ---\n",
    "session = None\n",
    "engine = None\n",
    "\n",
    "print(\"\\n--- Testing search_vdb() ---\")\n",
    "\n",
    "try:\n",
    "    # Check if necessary components are available\n",
    "    if INIT_DB_AVAILABLE and SEARCH_AVAILABLE:\n",
    "        # 1. Connect to the database\n",
    "        print(\"Connecting to database...\")\n",
    "        connect_start_time = time.time()\n",
    "        session, engine = init_vector_db(wipe_database=False)\n",
    "\n",
    "        if engine: # Check if connection was successful\n",
    "            print(f\"   Connected in {time.time() - connect_start_time:.2f}s\")\n",
    "\n",
    "            # 2. Perform the search\n",
    "            print(f\"\\nPerforming search for: '{SEARCH_QUERY}' (Top {NUM_RESULTS} results)\")\n",
    "            search_start_time = time.time()\n",
    "            try:\n",
    "                search_results = search_vdb(SEARCH_QUERY, num_results=NUM_RESULTS)\n",
    "                print(f\"   Search completed in {time.time() - search_start_time:.2f}s\")\n",
    "                print(f\"   Found {len(search_results)} results.\")\n",
    "\n",
    "                # 3. Display the results\n",
    "                print(\"\\n--- Search Results (Formatted JSON) ---\")\n",
    "                if search_results:\n",
    "                    # Use json.dumps for pretty printing\n",
    "                    print(json.dumps(search_results, indent=4))\n",
    "                else:\n",
    "                    print(\"   No results returned.\")\n",
    "\n",
    "            except Exception as search_error:\n",
    "                print(f\"   ❌ Error during search_vdb(): {search_error}\")\n",
    "\n",
    "        else:\n",
    "             print(\"   ❌ Failed to establish database connection via init_vector_db.\")\n",
    "\n",
    "    else:\n",
    "        print(\"   Skipping test: Required functions (init_vector_db or search_vdb) not imported.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    # 4. Close the connection\n",
    "    if session and session.is_active:\n",
    "        session.close()\n",
    "        print(\"\\nDatabase session closed.\")\n",
    "    elif engine:\n",
    "         print(\"\\nDatabase connection closed (or session inactive).\")\n",
    "\n",
    "print(f\"\\n✅ Search test finished.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Review\n",
    "#\n",
    "# 1. Check the output above. Does it show the number of results you requested?\n",
    "# 2. Examine the structure and content of the results. Does it match the format you expect (list of lists/tuples with ID, scores, paths, content)?\n",
    "# 3. Are the results relevant to the search query?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ init_vector_db imported successfully.\n",
      "\n",
      "Connecting to database to fetch file paths from 'vector_db'...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   Connected in 0.01s\n",
      "\n",
      "[1/2] Checking 'vector_db' table and fetching file paths...\n",
      "   ✅ Table 'public.vector_db' exists.\n",
      "   Fetched 47822 file paths in 0.05s\n",
      "\n",
      "[2/2] Analyzing file extensions...\n",
      "\n",
      "   --- Top 6 File Extensions in 'vector_db' ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "extension\n",
       ".pdf     40089\n",
       ".doc      3206\n",
       ".jpg      3187\n",
       ".docx      913\n",
       ".gif       264\n",
       ".png       163\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database session closed.\n",
      "\n",
      "✅ File type distribution analysis finished.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Analyze File Type Distribution in `vector_db`\n",
    "#\n",
    "# This cell connects to the database, fetches the `filepath` data from the\n",
    "# `public.vector_db` table, extracts file extensions, and calculates\n",
    "# the distribution of different document types.\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os # For path manipulation (splitext)\n",
    "from sqlalchemy import text, inspect as sql_inspect\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path to point to your VDB pipeline location\n",
    "VDB_PIPELINE_PATH = \"/shared_folders/team_1/mark_vdb/vdb_pipeline\" #<-- ADJUST IF NEEDED\n",
    "TOP_N_TYPES = 20 # How many top file types to display\n",
    "\n",
    "# --- Add VDB pipeline to Python path ---\n",
    "if VDB_PIPELINE_PATH not in sys.path:\n",
    "    sys.path.append(VDB_PIPELINE_PATH)\n",
    "\n",
    "# --- Import database initializer ---\n",
    "try:\n",
    "    from init_vector_db import init_vector_db\n",
    "    INIT_DB_AVAILABLE = True\n",
    "    print(\"✅ init_vector_db imported successfully.\")\n",
    "except ImportError as e:\n",
    "    INIT_DB_AVAILABLE = False\n",
    "    warnings.warn(f\"⚠️ Failed to import init_vector_db. Check VDB_PIPELINE_PATH: {e}\")\n",
    "    def init_vector_db(wipe_database=False): return None, None\n",
    "\n",
    "# --- Connect, Fetch, and Analyze ---\n",
    "session = None\n",
    "engine = None\n",
    "df_paths = pd.DataFrame() # Initialize empty DataFrame\n",
    "\n",
    "print(\"\\nConnecting to database to fetch file paths from 'vector_db'...\")\n",
    "connect_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    if INIT_DB_AVAILABLE:\n",
    "        session, engine = init_vector_db(wipe_database=False)\n",
    "\n",
    "        if engine: # Check if connection was successful\n",
    "            print(f\"   Connected in {time.time() - connect_start_time:.2f}s\")\n",
    "            inspector = sql_inspect(engine)\n",
    "\n",
    "            # --- 1) Check Table and Fetch File Paths ---\n",
    "            print(\"\\n[1/2] Checking 'vector_db' table and fetching file paths...\")\n",
    "            table_name = \"vector_db\"\n",
    "            if inspector.has_table(table_name, schema=\"public\"):\n",
    "                print(f\"   ✅ Table 'public.{table_name}' exists.\")\n",
    "                # Check if filepath column exists\n",
    "                columns_info = inspector.get_columns(table_name, schema=\"public\")\n",
    "                existing_columns = [col['name'] for col in columns_info] if columns_info else []\n",
    "\n",
    "                if 'markdown' in existing_columns:\n",
    "                    fetch_start_time = time.time()\n",
    "                    query_select = text(f\"\"\"\n",
    "                        SELECT markdown\n",
    "                        FROM public.{table_name}\n",
    "                        WHERE markdown IS NOT NULL;\n",
    "                    \"\"\")\n",
    "                    # Fetch all paths. Use chunking if memory is a concern for very large tables.\n",
    "                    df_paths = pd.read_sql(query_select, con=engine)\n",
    "                    print(f\"   Fetched {len(df_paths)} file paths in {time.time() - fetch_start_time:.2f}s\")\n",
    "\n",
    "                    if not df_paths.empty:\n",
    "                        # --- 2) Analyze File Extensions ---\n",
    "                        print(\"\\n[2/2] Analyzing file extensions...\")\n",
    "\n",
    "                        # Extract extension, convert to lowercase, handle missing extensions\n",
    "                        # Use os.path.splitext to correctly handle extensions\n",
    "                        df_paths['extension'] = df_paths['markdown'].apply(\n",
    "                            lambda x: os.path.splitext(x)[1].lower() if pd.notnull(x) and isinstance(x, str) else ''\n",
    "                        )\n",
    "                        extension_counts = df_paths['extension'].value_counts()\n",
    "\n",
    "                        print(f\"\\n   --- Top {min(TOP_N_TYPES, len(extension_counts))} File Extensions in '{table_name}' ---\")\n",
    "                        display(extension_counts.head(TOP_N_TYPES))\n",
    "\n",
    "                    else:\n",
    "                        print(\"   No non-NULL file paths found in the table to analyze.\")\n",
    "                else:\n",
    "                     print(f\"   ❌ Column 'filepath' not found in table '{table_name}'.\")\n",
    "            else:\n",
    "                 print(f\"   ❌ Table 'public.{table_name}' does not exist.\")\n",
    "\n",
    "        else:\n",
    "             print(\"   ❌ Failed to establish database connection via init_vector_db.\")\n",
    "\n",
    "    else:\n",
    "        print(\"   Skipping database connection as init_vector_db was not imported.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    # --- Close Connection ---\n",
    "    if session and session.is_active:\n",
    "        session.close()\n",
    "        print(\"\\nDatabase session closed.\")\n",
    "    elif engine:\n",
    "         print(\"\\nDatabase connection closed (or session inactive).\")\n",
    "\n",
    "print(f\"\\n✅ File type distribution analysis finished.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Review\n",
    "#\n",
    "# 1. Examine the list of file extensions and their counts.\n",
    "# 2. This shows the distribution of original document types represented by the chunks in the `vector_db` table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ init_vector_db imported successfully.\n",
      "\n",
      "--- Database Schema Inspector ---\n",
      "\n",
      "Connecting to database to inspect schema...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   Connected in 0.01s\n",
      "\n",
      "Inspecting tables in 'public' schema...\n",
      "   Found tables: spatial_ref_sys, vector_db, summary_vectors, mock_items_2, vector_db_2\n",
      "\n",
      "--- Schema for table: public.spatial_ref_sys ---\n",
      "   • srid                 INTEGER                   NOT NULL\n",
      "   • auth_name            VARCHAR(256)              NULLABLE\n",
      "   • auth_srid            INTEGER                   NULLABLE\n",
      "   • srtext               VARCHAR(2048)             NULLABLE\n",
      "   • proj4text            VARCHAR(2048)             NULLABLE\n",
      "\n",
      "--- Schema for table: public.vector_db ---\n",
      "   • id                   INTEGER                   NOT NULL\n",
      "   • chunkid              INTEGER                   NULLABLE\n",
      "   • description          TEXT                      NULLABLE\n",
      "   • category             TEXT                      NULLABLE\n",
      "   • md                   JSONB                     NULLABLE\n",
      "   • filepath             TEXT                      NULLABLE\n",
      "   • markdown             TEXT                      NULLABLE\n",
      "   • embedding            HALFVEC(768)              NULLABLE\n",
      "\n",
      "--- Schema for table: public.summary_vectors ---\n",
      "   • doc_id               INTEGER                   NOT NULL\n",
      "   • summary              TEXT                      NULLABLE\n",
      "   • file_path            TEXT                      NULLABLE\n",
      "   • embedding            VECTOR(384)               NULLABLE\n",
      "   • cluster_id           INTEGER                   NULLABLE\n",
      "   • cluster_name         TEXT                      NULLABLE\n",
      "\n",
      "--- Schema for table: public.mock_items_2 ---\n",
      "   • id                   INTEGER                   NOT NULL\n",
      "   • description          TEXT                      NULLABLE\n",
      "   • rating               INTEGER                   NULLABLE\n",
      "   • category             TEXT                      NULLABLE\n",
      "   • in_stock             BOOLEAN                   NULLABLE\n",
      "   • test_field           BOOLEAN                   NOT NULL\n",
      "   • md                   JSONB                     NULLABLE\n",
      "   • embedding            HALFVEC(1024)             NULLABLE\n",
      "\n",
      "--- Schema for table: public.vector_db_2 ---\n",
      "   • id                   INTEGER                   NOT NULL\n",
      "   • description          TEXT                      NULLABLE\n",
      "   • rating               INTEGER                   NULLABLE\n",
      "   • category             TEXT                      NULLABLE\n",
      "   • md                   JSONB                     NULLABLE\n",
      "   • file_path            TEXT                      NULLABLE\n",
      "   • markdown             TEXT                      NULLABLE\n",
      "   • embedding            HALFVEC(1024)             NULLABLE\n",
      "\n",
      "Database session closed.\n",
      "\n",
      "✅ Schema inspection finished.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "inspect_db_schema.py\n",
    "\n",
    "Connects to the database specified by the VDB pipeline configuration\n",
    "and prints the schema (tables, columns, types) found in the 'public' schema.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from sqlalchemy import create_engine, text, inspect as sql_inspect\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path to point to your VDB pipeline location\n",
    "VDB_PIPELINE_PATH = \"/shared_folders/team_1/mark_vdb/vdb_pipeline\" #<-- ADJUST IF NEEDED\n",
    "\n",
    "# --- Add VDB pipeline to Python path ---\n",
    "if VDB_PIPELINE_PATH not in sys.path:\n",
    "    sys.path.append(VDB_PIPELINE_PATH)\n",
    "\n",
    "# --- Import database initializer ---\n",
    "try:\n",
    "    # Assuming init_vector_db returns session, engine\n",
    "    from init_vector_db import init_vector_db\n",
    "    INIT_DB_AVAILABLE = True\n",
    "    print(\"✅ init_vector_db imported successfully.\")\n",
    "except ImportError as e:\n",
    "    INIT_DB_AVAILABLE = False\n",
    "    warnings.warn(f\"⚠️ Failed to import init_vector_db. Check VDB_PIPELINE_PATH: {e}\")\n",
    "    # Define a placeholder if needed, or just rely on the check later\n",
    "    def init_vector_db(wipe_database=False): return None, None\n",
    "\n",
    "# --- Main Execution Function ---\n",
    "def main():\n",
    "    \"\"\"Connects to the DB and prints the schema.\"\"\"\n",
    "    session = None\n",
    "    engine = None\n",
    "\n",
    "    print(\"\\n--- Database Schema Inspector ---\")\n",
    "\n",
    "    if not INIT_DB_AVAILABLE:\n",
    "        print(\"❌ Cannot proceed: init_vector_db failed to import.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nConnecting to database to inspect schema...\")\n",
    "    connect_start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        session, engine = init_vector_db(wipe_database=False)\n",
    "\n",
    "        if not engine: # Check if connection was successful\n",
    "            print(\"   ❌ Failed to establish database connection via init_vector_db.\")\n",
    "            return # Exit if connection failed\n",
    "\n",
    "        print(f\"   Connected in {time.time() - connect_start_time:.2f}s\")\n",
    "        inspector = sql_inspect(engine)\n",
    "\n",
    "        # --- Get List of Tables ---\n",
    "        print(\"\\nInspecting tables in 'public' schema...\")\n",
    "        table_names = inspector.get_table_names(schema='public')\n",
    "\n",
    "        if not table_names:\n",
    "            print(\"   No tables found in the 'public' schema.\")\n",
    "        else:\n",
    "            print(f\"   Found tables: {', '.join(table_names)}\")\n",
    "\n",
    "            # --- Inspect Columns for Each Table ---\n",
    "            for table_name in table_names:\n",
    "                print(f\"\\n--- Schema for table: public.{table_name} ---\")\n",
    "                try:\n",
    "                    columns = inspector.get_columns(table_name, schema='public')\n",
    "                    if columns:\n",
    "                        for column in columns:\n",
    "                            col_name = column['name']\n",
    "                            # Use .__str__() for robust type representation\n",
    "                            col_type = column['type'].__str__()\n",
    "                            col_nullable = column['nullable']\n",
    "                            # Basic formatting for alignment\n",
    "                            print(f\"   • {col_name:<20} {col_type:<25} {'NULLABLE' if col_nullable else 'NOT NULL'}\")\n",
    "                    else:\n",
    "                        print(\"      No columns found for this table.\")\n",
    "                except Exception as col_error:\n",
    "                     print(f\"      ⚠️ Error inspecting columns for {table_name}: {col_error}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        # --- Close Connection ---\n",
    "        if session and session.is_active:\n",
    "            session.close()\n",
    "            print(\"\\nDatabase session closed.\")\n",
    "        elif engine: # If engine exists but session might not have been active\n",
    "             print(\"\\nDatabase connection closed (or session inactive).\")\n",
    "\n",
    "    print(f\"\\n✅ Schema inspection finished.\")\n",
    "\n",
    "\n",
    "# --- Script Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ init_vector_db imported successfully.\n",
      "\n",
      "--- Fetching Full Sample Row from 'vector_db' ---\n",
      "Connecting to database...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   Connected in 0.01s\n",
      "\n",
      "Checking table 'public.vector_db'...\n",
      "   ✅ Table and columns found.\n",
      "\n",
      "Fetching sample row (offset 0)...\n",
      "   Query completed in 0.00s\n",
      "\n",
      "--- Full Sample Row Data ---\n",
      "\n",
      "-- Column: id --\n",
      "  Type: int, Value: 1\n",
      "\n",
      "-- Column: chunkid --\n",
      "  Type: int, Value: 0\n",
      "\n",
      "-- Column: description --\n",
      "  Type: str, Preview (first 500 chars):\n",
      "the provided image appears to be a technical drawing , likely a schematic diagram for an electrical or mechanical system . here is a detailed analysis : # # # layout and structure : - the drawing is divided into two main sections , each containing multiple components arranged in a grid-like pattern . - each component is connected by lines that represent electrical or mechanical connections . - there are labels and annotations next to each component , indicating their names or functions . # # # c...\n",
      "\n",
      "-- Column: category --\n",
      "  Type: str, Value: \n",
      "\n",
      "-- Column: md --\n",
      "  Type: dict, Content:\n",
      "{}\n",
      "\n",
      "-- Column: filepath --\n",
      "  Type: str, Value: /shared_folders/team_1/ben/cleaned_data/20111004090335437_0005.md\n",
      "\n",
      "-- Column: markdown --\n",
      "  Type: str, Preview (first 500 chars):\n",
      "/shared_folders/team_1/document_batch/UTILS/Library/Drawings/JFDP Sketches/JFDP Project Sketches/Demin System/20111004090335437_0005.pdf\n",
      "\n",
      "-- Column: embedding --\n",
      "  Type: str, Preview: [-0.024612427,-0.05895996,-0.016738892,0.081848145,-0.027282715,0.054840088,-0.009284973,0.06463623,...0.033416748,0.016845703,0.0017108917,-0.008285522,0.009254456,-0.040924072,-0.08660889,0.0018091202]\n",
      "\n",
      "Database session closed.\n",
      "\n",
      "✅ Sample row display finished.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "show_full_vector_db_row.py\n",
    "\n",
    "Connects to the database and fetches a single sample row from the\n",
    "`vector_db` table, displaying the full value (or a preview) for each column.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import text, inspect as sql_inspect\n",
    "import warnings\n",
    "import json # For potentially pretty-printing JSONB\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path to point to your VDB pipeline location\n",
    "VDB_PIPELINE_PATH = \"/shared_folders/team_1/mark_vdb/vdb_pipeline\" #<-- ADJUST IF NEEDED\n",
    "TABLE_NAME = \"vector_db\" # The table to inspect\n",
    "ROW_OFFSET = 0 # Fetch the first row (change to fetch a different one, e.g., 10)\n",
    "\n",
    "# --- Add VDB pipeline to Python path ---\n",
    "if VDB_PIPELINE_PATH not in sys.path:\n",
    "    sys.path.append(VDB_PIPELINE_PATH)\n",
    "\n",
    "# --- Import database initializer ---\n",
    "try:\n",
    "    from init_vector_db import init_vector_db\n",
    "    INIT_DB_AVAILABLE = True\n",
    "    print(\"✅ init_vector_db imported successfully.\")\n",
    "except ImportError as e:\n",
    "    INIT_DB_AVAILABLE = False\n",
    "    warnings.warn(f\"⚠️ Failed to import init_vector_db. Check VDB_PIPELINE_PATH: {e}\")\n",
    "    def init_vector_db(wipe_database=False): return None, None\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    \"\"\"Main function to fetch and display a sample row.\"\"\"\n",
    "    session = None\n",
    "    engine = None\n",
    "    print(f\"\\n--- Fetching Full Sample Row from '{TABLE_NAME}' ---\")\n",
    "\n",
    "    if not INIT_DB_AVAILABLE:\n",
    "        print(\"❌ Cannot proceed: init_vector_db failed to import.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 1. Connect\n",
    "        print(\"Connecting to database...\")\n",
    "        connect_start_time = time.time()\n",
    "        session, engine = init_vector_db(wipe_database=False)\n",
    "\n",
    "        if not engine:\n",
    "            print(\"   ❌ Failed to establish database connection.\")\n",
    "            return\n",
    "        print(f\"   Connected in {time.time() - connect_start_time:.2f}s\")\n",
    "        inspector = sql_inspect(engine)\n",
    "\n",
    "        # 2. Check Table and Get Columns\n",
    "        print(f\"\\nChecking table 'public.{TABLE_NAME}'...\")\n",
    "        if not inspector.has_table(TABLE_NAME, schema=\"public\"):\n",
    "             print(f\"   ❌ Table 'public.{TABLE_NAME}' does not exist.\")\n",
    "             return\n",
    "\n",
    "        columns_info = inspector.get_columns(TABLE_NAME, schema=\"public\")\n",
    "        existing_columns = [col['name'] for col in columns_info] if columns_info else []\n",
    "\n",
    "        if not existing_columns:\n",
    "            print(f\"   ❌ Could not retrieve columns for table '{TABLE_NAME}'.\")\n",
    "            return\n",
    "        print(f\"   ✅ Table and columns found.\")\n",
    "\n",
    "        # 3. Fetch Sample Row\n",
    "        print(f\"\\nFetching sample row (offset {ROW_OFFSET})...\")\n",
    "        fetch_start_time = time.time()\n",
    "        sample_row = None\n",
    "        try:\n",
    "            # Select all columns, order by ID, use OFFSET and LIMIT\n",
    "            select_cols_str = \", \".join([f'\"{col}\"' for col in existing_columns]) # Quote names\n",
    "            query = text(f\"\"\"\n",
    "                SELECT {select_cols_str}\n",
    "                FROM public.{TABLE_NAME}\n",
    "                ORDER BY id -- Assuming 'id' is the primary key or an indexed column\n",
    "                LIMIT 1 OFFSET :offset;\n",
    "            \"\"\")\n",
    "            result = session.execute(query, {'offset': ROW_OFFSET}).fetchone() # Fetch exactly one row\n",
    "            if result:\n",
    "                 sample_row = result._mapping # Access columns by name using mapping proxy\n",
    "            print(f\"   Query completed in {time.time() - fetch_start_time:.2f}s\")\n",
    "\n",
    "        except Exception as query_error:\n",
    "            print(f\"   ❌ Error executing query: {query_error}\")\n",
    "\n",
    "        # 4. Display Row Content\n",
    "        print(\"\\n--- Full Sample Row Data ---\")\n",
    "        if sample_row:\n",
    "            for col_name in existing_columns:\n",
    "                value = sample_row.get(col_name) # Get value by name\n",
    "                print(f\"\\n-- Column: {col_name} --\")\n",
    "                # Provide previews for potentially very long fields\n",
    "                if col_name == 'embedding':\n",
    "                    value_str = str(value)\n",
    "                    preview = value_str[:100] + \"...\" + value_str[-100:] if len(value_str) > 200 else value_str\n",
    "                    print(f\"  Type: {type(value).__name__}, Preview: {preview}\")\n",
    "                elif col_name in ['markdown', 'description'] and isinstance(value, str):\n",
    "                    preview = value[:500] + \"...\" if len(value) > 500 else value\n",
    "                    print(f\"  Type: {type(value).__name__}, Preview (first 500 chars):\")\n",
    "                    print(preview)\n",
    "                elif col_name == 'md' and value is not None: # Pretty print JSONB\n",
    "                     try:\n",
    "                         print(f\"  Type: {type(value).__name__}, Content:\")\n",
    "                         print(json.dumps(value, indent=2)) # Pretty print JSON\n",
    "                     except TypeError:\n",
    "                         print(f\"  Value (non-JSON serializable): {value}\")\n",
    "                else:\n",
    "                    # Print other types directly\n",
    "                    print(f\"  Type: {type(value).__name__}, Value: {value}\")\n",
    "        else:\n",
    "            print(f\"   No row found at offset {ROW_OFFSET} or query failed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        # 5. Close connection\n",
    "        if session and session.is_active:\n",
    "            session.close()\n",
    "            print(\"\\nDatabase session closed.\")\n",
    "        elif engine:\n",
    "             print(\"\\nDatabase connection closed (or session inactive).\")\n",
    "\n",
    "    print(f\"\\n✅ Sample row display finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ init_vector_db imported successfully.\n",
      "\n",
      "--- Finding Document with Multiple Chunks in 'vector_db' ---\n",
      "Connecting to database...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   Connected in 0.01s\n",
      "\n",
      "Checking table 'public.vector_db'...\n",
      "   ✅ Table and required columns exist.\n",
      "\n",
      "Finding a document path (markdown) with more than one chunk...\n",
      "   Found example path: /shared_folders/team_1/document_batch/UTILS/UtInfo/001 Staff Folders/_Past Employees/McClay, Charles/DEMIN/Demin 3 Project/1998 D2 Project Files/1998 Demineralization Water Expansion/Vol VI Book 12/Book 12 Tab 6/20110404125325_001.PDF\n",
      "   Path search completed in 0.03s\n",
      "\n",
      "Fetching up to 10 chunks for path: /shared_folders/team_1/document_batch/UTILS/UtInfo/001 Staff Folders/_Past Employees/McClay, Charles/DEMIN/Demin 3 Project/1998 D2 Project Files/1998 Demineralization Water Expansion/Vol VI Book 12/Book 12 Tab 6/20110404125325_001.PDF\n",
      "   Query completed in 0.01s\n",
      "   Fetched 4 chunks (display limited to 10).\n",
      "\n",
      "--- Chunks for the Selected Document ---\n",
      "\n",
      "--- Chunk 1 (ID: 43850, ChunkID: 10068) ---\n",
      "  Content Preview:\n",
      "    `` ` notes : 1 , all sag rods ~ `` oia bar . see plan 2. w12 indicates wi2x26 c8 indicates c8x11.5 girt line 4-~ `` dia eq sag line rod sp . ( typ ) `` ` w12 * * cmu wall * * `` ` ( see arcph~ dwgs , typ ) elevation at column line b girt # # # # # # line el . 28 ’ -6 `` bibase el . elevation at colu...\n",
      "--------------------\n",
      "\n",
      "--- Chunk 2 (ID: 43851, ChunkID: 10068) ---\n",
      "  Content Preview:\n",
      "    include `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `` x 12 `` , `` 12 `...\n",
      "--------------------\n",
      "\n",
      "--- Chunk 3 (ID: 43852, ChunkID: 10068) ---\n",
      "  Content Preview:\n",
      "    a series of labeled components such as `` a , `` `` b , `` `` c , `` etc . , which seem to represent different parts of the system . - there are also numerical labels like `` 1 , `` `` 2 , `` `` 3 , `` etc . , which could indicate the sequence or order of these components . 2 . * * middle section * ...\n",
      "--------------------\n",
      "\n",
      "--- Chunk 4 (ID: 43853, ChunkID: 10068) ---\n",
      "  Content Preview:\n",
      "    -- - the provided image appears to be a technical drawing , likely a schematic or an assembly drawing for a mechanical or electrical component . here is a detailed analysis : # # # layout and structure : - the drawing is divided into several sections , each containing different components or parts o...\n",
      "--------------------\n",
      "\n",
      "Database session closed.\n",
      "\n",
      "✅ Script finished.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "find_multi_chunk_doc.py\n",
    "\n",
    "Connects to the database, identifies an original document path associated\n",
    "with multiple chunks in the `vector_db` table, and displays the details\n",
    "(ID, Chunk ID, Content Preview) for each of those chunks.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from sqlalchemy import text, inspect as sql_inspect\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path to point to your VDB pipeline location\n",
    "VDB_PIPELINE_PATH = \"/shared_folders/team_1/mark_vdb/vdb_pipeline\" #<-- ADJUST IF NEEDED\n",
    "TABLE_NAME = \"vector_db\" # The table containing chunks\n",
    "# Column likely representing the original document path (based on previous analysis)\n",
    "ORIGINAL_DOC_PATH_COLUMN = \"markdown\"\n",
    "# How many chunks to display for the found document\n",
    "MAX_CHUNKS_TO_DISPLAY = 10\n",
    "\n",
    "# --- Add VDB pipeline to Python path ---\n",
    "if VDB_PIPELINE_PATH not in sys.path:\n",
    "    sys.path.append(VDB_PIPELINE_PATH)\n",
    "\n",
    "# --- Import database initializer ---\n",
    "try:\n",
    "    from init_vector_db import init_vector_db\n",
    "    INIT_DB_AVAILABLE = True\n",
    "    print(\"✅ init_vector_db imported successfully.\")\n",
    "except ImportError as e:\n",
    "    INIT_DB_AVAILABLE = False\n",
    "    warnings.warn(f\"⚠️ Failed to import init_vector_db. Check VDB_PIPELINE_PATH: {e}\")\n",
    "    def init_vector_db(wipe_database=False): return None, None\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    \"\"\"Finds a document with multiple chunks and displays them.\"\"\"\n",
    "    session = None\n",
    "    engine = None\n",
    "    print(f\"\\n--- Finding Document with Multiple Chunks in '{TABLE_NAME}' ---\")\n",
    "\n",
    "    if not INIT_DB_AVAILABLE:\n",
    "        print(\"❌ Cannot proceed: init_vector_db failed to import.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 1. Connect\n",
    "        print(\"Connecting to database...\")\n",
    "        connect_start_time = time.time()\n",
    "        session, engine = init_vector_db(wipe_database=False)\n",
    "\n",
    "        if not engine:\n",
    "            print(\"   ❌ Failed to establish database connection.\")\n",
    "            return\n",
    "        print(f\"   Connected in {time.time() - connect_start_time:.2f}s\")\n",
    "        inspector = sql_inspect(engine)\n",
    "\n",
    "        # 2. Check Table and Columns\n",
    "        print(f\"\\nChecking table 'public.{TABLE_NAME}'...\")\n",
    "        if not inspector.has_table(TABLE_NAME, schema=\"public\"):\n",
    "             print(f\"   ❌ Table 'public.{TABLE_NAME}' does not exist.\")\n",
    "             return\n",
    "\n",
    "        columns_info = inspector.get_columns(TABLE_NAME, schema=\"public\")\n",
    "        existing_columns = [col['name'] for col in columns_info] if columns_info else []\n",
    "        required_cols = ['id', 'chunkid', 'description', ORIGINAL_DOC_PATH_COLUMN]\n",
    "\n",
    "        if not all(col in existing_columns for col in required_cols):\n",
    "             missing = [col for col in required_cols if col not in existing_columns]\n",
    "             print(f\"   ❌ Required columns missing from table: {', '.join(missing)}\")\n",
    "             return\n",
    "        print(\"   ✅ Table and required columns exist.\")\n",
    "\n",
    "        # 3. Find a Document Path with Multiple Chunks\n",
    "        print(f\"\\nFinding a document path ({ORIGINAL_DOC_PATH_COLUMN}) with more than one chunk...\")\n",
    "        find_path_start_time = time.time()\n",
    "        target_path = None\n",
    "        try:\n",
    "            # Query to group by the path column and count occurrences, finding one with count > 1\n",
    "            query_find = text(f\"\"\"\n",
    "                SELECT {ORIGINAL_DOC_PATH_COLUMN}\n",
    "                FROM public.{TABLE_NAME}\n",
    "                WHERE {ORIGINAL_DOC_PATH_COLUMN} IS NOT NULL\n",
    "                GROUP BY {ORIGINAL_DOC_PATH_COLUMN}\n",
    "                HAVING COUNT(*) > 1\n",
    "                LIMIT 1;\n",
    "            \"\"\")\n",
    "            result = session.execute(query_find).scalar_one_or_none() # Get the first path found\n",
    "            if result:\n",
    "                 target_path = result\n",
    "                 print(f\"   Found example path: {target_path}\")\n",
    "            else:\n",
    "                 print(\"   Could not find any document paths with multiple chunks.\")\n",
    "\n",
    "            print(f\"   Path search completed in {time.time() - find_path_start_time:.2f}s\")\n",
    "\n",
    "        except Exception as find_error:\n",
    "            print(f\"   ❌ Error finding multi-chunk document: {find_error}\")\n",
    "\n",
    "        # 4. Fetch and Display Chunks for the Found Path\n",
    "        if target_path:\n",
    "            print(f\"\\nFetching up to {MAX_CHUNKS_TO_DISPLAY} chunks for path: {target_path}\")\n",
    "            fetch_chunks_start_time = time.time()\n",
    "            chunks = []\n",
    "            try:\n",
    "                # Query to get all chunks for the specific path\n",
    "                query_fetch = text(f\"\"\"\n",
    "                    SELECT id, chunkid, description\n",
    "                    FROM public.{TABLE_NAME}\n",
    "                    WHERE {ORIGINAL_DOC_PATH_COLUMN} = :target_path\n",
    "                    ORDER BY chunkid ASC -- Order by chunk sequence\n",
    "                    LIMIT :limit;\n",
    "                \"\"\")\n",
    "                results = session.execute(query_fetch, {\n",
    "                    'target_path': target_path,\n",
    "                    'limit': MAX_CHUNKS_TO_DISPLAY\n",
    "                }).fetchall()\n",
    "                chunks = results\n",
    "                print(f\"   Query completed in {time.time() - fetch_chunks_start_time:.2f}s\")\n",
    "                print(f\"   Fetched {len(chunks)} chunks (display limited to {MAX_CHUNKS_TO_DISPLAY}).\")\n",
    "\n",
    "            except Exception as fetch_error:\n",
    "                print(f\"   ❌ Error fetching chunks: {fetch_error}\")\n",
    "\n",
    "            # Display the fetched chunks\n",
    "            print(\"\\n--- Chunks for the Selected Document ---\")\n",
    "            if chunks:\n",
    "                for i, row in enumerate(chunks):\n",
    "                    row_dict = row._mapping # Access columns by name\n",
    "                    doc_id = row_dict.get('id')\n",
    "                    chunk_id = row_dict.get('chunkid')\n",
    "                    content = row_dict.get('description') # Use description as content based on prior analysis\n",
    "\n",
    "                    print(f\"\\n--- Chunk {i+1} (ID: {doc_id}, ChunkID: {chunk_id}) ---\")\n",
    "                    print(f\"  Content Preview:\")\n",
    "                    print(f\"    {content[:300] if isinstance(content, str) else 'N/A'}...\") # Preview content\n",
    "                    print(\"-\" * 20)\n",
    "            else:\n",
    "                print(\"   No chunks found for the selected path or query failed.\")\n",
    "        else:\n",
    "            print(\"\\nSkipping chunk display as no multi-chunk document path was found.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        # 5. Close connection\n",
    "        if session and session.is_active:\n",
    "            session.close()\n",
    "            print(\"\\nDatabase session closed.\")\n",
    "        elif engine:\n",
    "             print(\"\\nDatabase connection closed (or session inactive).\")\n",
    "\n",
    "    print(f\"\\n✅ Script finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ init_vector_db imported successfully.\n",
      "\n",
      "--- Counting Unique Documents in 'vector_db' (Column: 'markdown') ---\n",
      "Connecting to database...\n",
      "CHECKING IF SEARCH INDICES CREATED: True\n",
      "   Connected in 0.01s\n",
      "\n",
      "Checking table 'public.vector_db' and column 'markdown'...\n",
      "   ✅ Table and specified column exist.\n",
      "\n",
      "Counting distinct values in 'markdown' column...\n",
      "   Query completed in 0.24s\n",
      "\n",
      "   📊 Found 13755 unique document paths in 'markdown'.\n",
      "\n",
      "Database session closed.\n",
      "\n",
      "✅ Unique document count finished.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "count_unique_docs.py\n",
    "\n",
    "Connects to the database and counts the number of unique original document paths\n",
    "stored in the specified column (`markdown` by default) of the `vector_db` table.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from sqlalchemy import text, inspect as sql_inspect\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path to point to your VDB pipeline location\n",
    "VDB_PIPELINE_PATH = \"/shared_folders/team_1/mark_vdb/vdb_pipeline\" #<-- ADJUST IF NEEDED\n",
    "TABLE_NAME = \"vector_db\" # The table containing chunks\n",
    "# Column representing the original document path (based on prior analysis)\n",
    "UNIQUE_DOC_COLUMN = \"markdown\"\n",
    "\n",
    "# --- Add VDB pipeline to Python path ---\n",
    "if VDB_PIPELINE_PATH not in sys.path:\n",
    "    sys.path.append(VDB_PIPELINE_PATH)\n",
    "\n",
    "# --- Import database initializer ---\n",
    "try:\n",
    "    from init_vector_db import init_vector_db\n",
    "    INIT_DB_AVAILABLE = True\n",
    "    print(\"✅ init_vector_db imported successfully.\")\n",
    "except ImportError as e:\n",
    "    INIT_DB_AVAILABLE = False\n",
    "    warnings.warn(f\"⚠️ Failed to import init_vector_db. Check VDB_PIPELINE_PATH: {e}\")\n",
    "    def init_vector_db(wipe_database=False): return None, None\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    \"\"\"Connects to the DB and counts unique document paths.\"\"\"\n",
    "    session = None\n",
    "    engine = None\n",
    "    print(f\"\\n--- Counting Unique Documents in '{TABLE_NAME}' (Column: '{UNIQUE_DOC_COLUMN}') ---\")\n",
    "\n",
    "    if not INIT_DB_AVAILABLE:\n",
    "        print(\"❌ Cannot proceed: init_vector_db failed to import.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 1. Connect\n",
    "        print(\"Connecting to database...\")\n",
    "        connect_start_time = time.time()\n",
    "        session, engine = init_vector_db(wipe_database=False)\n",
    "\n",
    "        if not engine:\n",
    "            print(\"   ❌ Failed to establish database connection.\")\n",
    "            return\n",
    "        print(f\"   Connected in {time.time() - connect_start_time:.2f}s\")\n",
    "        inspector = sql_inspect(engine)\n",
    "\n",
    "        # 2. Check Table and Column\n",
    "        print(f\"\\nChecking table 'public.{TABLE_NAME}' and column '{UNIQUE_DOC_COLUMN}'...\")\n",
    "        if not inspector.has_table(TABLE_NAME, schema=\"public\"):\n",
    "             print(f\"   ❌ Table 'public.{TABLE_NAME}' does not exist.\")\n",
    "             return\n",
    "\n",
    "        columns_info = inspector.get_columns(TABLE_NAME, schema=\"public\")\n",
    "        existing_columns = [col['name'] for col in columns_info] if columns_info else []\n",
    "\n",
    "        if UNIQUE_DOC_COLUMN not in existing_columns:\n",
    "             print(f\"   ❌ Column '{UNIQUE_DOC_COLUMN}' not found in table '{TABLE_NAME}'.\")\n",
    "             return\n",
    "        print(\"   ✅ Table and specified column exist.\")\n",
    "\n",
    "        # 3. Count Unique Document Paths\n",
    "        print(f\"\\nCounting distinct values in '{UNIQUE_DOC_COLUMN}' column...\")\n",
    "        count_start_time = time.time()\n",
    "        unique_count = 0\n",
    "        try:\n",
    "            # Use COUNT(DISTINCT column_name)\n",
    "            # Ensure the column name is handled safely if it contains special characters\n",
    "            # (though 'markdown' is standard and likely safe)\n",
    "            query = text(f\"\"\"\n",
    "                SELECT COUNT(DISTINCT \"{UNIQUE_DOC_COLUMN}\")\n",
    "                FROM public.\"{TABLE_NAME}\"\n",
    "                WHERE \"{UNIQUE_DOC_COLUMN}\" IS NOT NULL;\n",
    "            \"\"\")\n",
    "            result = session.execute(query).scalar_one_or_none()\n",
    "            unique_count = result if result is not None else 0\n",
    "            print(f\"   Query completed in {time.time() - count_start_time:.2f}s\")\n",
    "            print(f\"\\n   📊 Found {unique_count} unique document paths in '{UNIQUE_DOC_COLUMN}'.\")\n",
    "\n",
    "        except Exception as query_error:\n",
    "            print(f\"   ❌ Error executing count query: {query_error}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        # 4. Close connection\n",
    "        if session and session.is_active:\n",
    "            session.close()\n",
    "            print(\"\\nDatabase session closed.\")\n",
    "        elif engine:\n",
    "             print(\"\\nDatabase connection closed (or session inactive).\")\n",
    "\n",
    "    print(f\"\\n✅ Unique document count finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragui_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
